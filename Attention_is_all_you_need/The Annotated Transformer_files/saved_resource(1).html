<!DOCTYPE html>
<!-- saved from url=(0255)https://disqus.com/embed/comments/?base=default&f=harvard-nlp&t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&t_d=The%20Annotated%20Transformer&t_t=The%20Annotated%20Transformer&s_o=default#version=ebd3bed34ae3ee660f5521290afa665f -->
<html lang="en" dir="ltr" class="js no-touch localstorage sessionstorage contenteditable use-opacity-transitions embed-refresh embed-refresh-v2" style="--publisher-color: rgb(51,122,183); --publisher-color-safe: rgb(51,122,183);"><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Disqus Comments</title>

    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">

    <style>
        .alert--warning {
            border-radius: 3px;
            padding: 10px 15px;
            margin-bottom: 10px;
            background-color: #FFE070;
            color: #A47703;
        }

        .alert--warning a,
        .alert--warning a:hover,
        .alert--warning strong {
            color: #A47703;
            font-weight: bold;
        }

        .alert--error p,
        .alert--warning p {
            margin-top: 5px;
            margin-bottom: 5px;
        }
        
        </style>
    
    <style>
        
        html {
            overflow: hidden;
        }
        

        #error {
            display: none;
        }

        .clearfix:after {
            content: "";
            display: block;
            height: 0;
            clear: both;
            visibility: hidden;
        }

        
    </style>

<script src="./cb=gapi.loaded_0" async=""></script><script src="./sdk.js.다운로드" async="" crossorigin="anonymous"></script><script crossorigin="anonymous" id="bootstrap-script" data-app="lounge" src="./lounge.load.ebd3bed34ae3ee660f5521290afa665f.js.다운로드"></script><meta http-equiv="Content-Security-Policy" content="script-src https:;"><style>@font-face{font-family:icons;src:url(https://c.disquscdn.com/embedv2/latest/icons.woff2) format("woff2"),url(/assets/font/icons.woff) format("woff");font-weight:400;font-style:normal}._icon_1x9qx_7{display:inline-flex;align-items:center;justify-content:center}._icon_1x9qx_7:before{font-family:icons;content:var(--icon-content);speak:none;font-style:normal;font-weight:400;font-variant:normal;text-transform:none;line-height:1;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale}._success_16l9s_1,._info_16l9s_1,._warn_16l9s_1,._error_16l9s_1{display:flex;justify-content:space-between;padding:10px 14px;color:#fff;border-bottom:2px solid rgba(60,78,110,.18);font-size:13px}.dark ._success_16l9s_1,.dark ._info_16l9s_1,.dark ._warn_16l9s_1,.dark ._error_16l9s_1{border-bottom:2px solid rgba(255,255,255,.2)}._success_16l9s_1 a,._info_16l9s_1 a,._warn_16l9s_1 a,._error_16l9s_1 a{font-weight:700;text-decoration:underline}._error_16l9s_1,._warn_16l9s_1{background:#f05f70}._info_16l9s_1,._success_16l9s_1{background:rgb(46,159,255)}._message_16l9s_33{display:flex;align-items:center}._icon_16l9s_38{margin-right:.5em;width:13px;height:13px}._dismiss_16l9s_44{background:transparent;border:none;cursor:pointer;color:inherit;font-size:16px}._icon_rsxid_1{font-size:16px;color:#ffd34f}._avatar-square_st8h3_1,._avatar-rounded_st8h3_1,._avatar-circle_st8h3_1{font-size:36px;position:relative;display:flex;align-items:center;justify-content:center;overflow:hidden;padding:0}._inner_st8h3_11{font-style:normal;font-weight:600;flex:auto;display:flex;align-items:center;justify-content:center;width:100%;height:100%;line-height:1.2;color:#fff;background:var(--publisher-color, rgb(46, 159, 255))}._image_st8h3_25{width:100%;height:100%;border-radius:inherit}._avatar-circle_st8h3_1{border-radius:50%}._avatar-rounded_st8h3_1{border-radius:16px}._avatar-square_st8h3_1{border-radius:2px}._avatar-large_st8h3_43{width:52px;height:52px}._avatar-medium_st8h3_48{width:40px;height:40px}._avatar-small_st8h3_53{width:36px;height:36px}._avatar-xsmall_st8h3_58{font-size:16px;width:30px;height:30px}._link_1hoja_1{color:var(--publisher-color, rgb(46, 159, 255))}._button_8fv5d_1{font-size:12px;font-weight:700;font-family:inherit;display:inline-block;padding:8px 12px;line-height:normal;border:initial;border-radius:3px;transition:all .25s ease-in-out;cursor:pointer;color:inherit}._button-fill_8fv5d_15{border-color:#687a86;background-color:#687a86;color:#fff}@media (hover: hover){._button-fill_8fv5d_15:hover{border-color:#526069;background-color:#526069;color:#fff}._button-fill_8fv5d_15:disabled,._button-fill_8fv5d_15:disabled:hover{cursor:default;border-color:#bcc5cb;background:#bcc5cb}.dark ._button-fill_8fv5d_15:disabled,.dark ._button-fill_8fv5d_15:disabled:hover{border-color:#ffffff80;background:rgba(255,255,255,.5);color:#ffffffb3}}._button-primary_8fv5d_38{border-color:#2e9fff;background-color:#2e9fff;color:#fff}@media (hover: hover){._button-primary_8fv5d_38:hover{border-color:#0087fa;background-color:#0087fa;color:#fff}._button-primary_8fv5d_38:disabled,._button-primary_8fv5d_38:disabled:hover{cursor:default;border-color:#c7e5ff;background:#c7e5ff}.dark ._button-primary_8fv5d_38:disabled,.dark ._button-primary_8fv5d_38:disabled:hover{border-color:#ffffff80;background:rgba(255,255,255,.5);color:#ffffffb3}}._button-danger_8fv5d_61{border-color:#f05f70;background-color:#f05f70;color:#fff}@media (hover: hover){._button-danger_8fv5d_61:hover{border-color:#ec3046;background-color:#ec3046;color:#fff}._button-danger_8fv5d_61:disabled,._button-danger_8fv5d_61:disabled:hover{cursor:default;border-color:#fdebed;background:#fdebed}.dark ._button-danger_8fv5d_61:disabled,.dark ._button-danger_8fv5d_61:disabled:hover{border-color:#ffffff80;background:rgba(255,255,255,.5);color:#ffffffb3}}._button-success_8fv5d_84{border-color:#5cb767;background-color:#5cb767;color:#fff}@media (hover: hover){._button-success_8fv5d_84:hover{border-color:#459b4f;background-color:#459b4f;color:#fff}._button-success_8fv5d_84:disabled,._button-success_8fv5d_84:disabled:hover{cursor:default;border-color:#c6e6ca;background:#c6e6ca}.dark ._button-success_8fv5d_84:disabled,.dark ._button-success_8fv5d_84:disabled:hover{border-color:#ffffff80;background:rgba(255,255,255,.5);color:#ffffffb3}}._card_hu9ay_1{overflow:hidden;color:#494e58;border:1px solid rgb(194,198,204);border-radius:5px;transition:all .25s ease-in-out}.dark ._card_hu9ay_1{color:#ffffffd9;border-color:#fff3}._content_zxk21_1{padding:12px 18px}._header_1iy7u_1{padding:10px 12px;border-bottom:1px solid rgb(235,238,242)}.dark ._header_1iy7u_1{border-color:#fff3}._heading_1iy7u_9{font-size:14px;font-weight:500;margin:0}._footer_pq6g3_1{display:flex;gap:10px;margin:0;padding:10px 18px 12px;border-top:1px solid rgb(235,238,242)}.dark ._footer_pq6g3_1{border-color:#fff3}._checkbox_1nula_1{display:inline-flex;align-items:center;gap:6px;line-height:1.3}._input_1nula_8{margin:0}._link_7oyne_1{color:inherit}._icon_1rgg8_1{margin-right:4px}._domain_1rgg8_5{font-size:11px;color:#656c7a}._loader_e4dlg_1{width:100%;height:var(--loader-height);text-align:center;background:url(data:image/gif;base64,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) no-repeat center center}._root_169ev_1{position:relative}._iframe_169ev_5{display:block;opacity:1;transition:opacity .5s ease-in-out}._iframe-loading_169ev_11{position:absolute;opacity:0;visibility:hidden}._image_1mcab_1{display:block;max-width:100%;max-height:480px;border-radius:3px}._root_1wfe0_1{position:relative;width:-webkit-max-content;width:-moz-max-content;width:max-content;max-width:100%}._container_1r7qu_1{min-height:100px}._mention_1xkh2_1{font-weight:700}._mention_1xkh2_1:before{content:"@"}.dark ._mention_1xkh2_1:before{color:#fff}._menu_1hdnv_1{border:2px solid #687a86;border-radius:15px 3px 15px 15px;background:#fff;outline:none}._button_1hdnv_8{display:inline-flex;align-items:center;justify-content:center;padding:0;color:#494e58;border:none;background:transparent;outline:none;cursor:pointer}._button_1hdnv_8:hover,._button_1hdnv_8[aria-expanded=true]{color:var(--publisher-color, rgb(46, 159, 255))}._item_2hnsa_1{font-size:11px;font-family:inherit;display:flex;padding:6px 8px;width:100%;color:#687a86;border:none;background:transparent;outline:none;cursor:pointer}._item_2hnsa_1:hover:not(:disabled),._item_2hnsa_1:focus:not(:disabled){color:var(--publisher-color, rgb(46, 159, 255))}._radio_1lyat_1{display:inline-flex;align-items:center;gap:6px;line-height:1.3}._input_1lyat_8{margin:0}._stars-inactive_1axb3_1,._stars-active_1axb3_1{display:flex;font-size:14px}._star-inactive_1axb3_6,._star-active_1axb3_6{font-size:14px}._root_1axb3_10{line-height:20px;display:inline-block;unicode-bidi:bidi-override;direction:ltr;position:relative}._stars-active_1axb3_1{position:absolute;z-index:1;top:0;left:0;width:var(--active-width);overflow:hidden;white-space:nowrap}._stars-inactive_1axb3_1{z-index:0}._star-active_1axb3_6{color:#ffd34f}._star-inactive_1axb3_6{color:#ebeef2}._spacer_1fr9r_1{display:inline-block;width:var(--spacer-width)}._spoiler_1mxys_1{display:inline;background:rgb(127,145,158);color:transparent;padding:0 .5em}._spoiler_1mxys_1 a{visibility:hidden;transition:none}._spoiler_1mxys_1:hover,._spoiler_1mxys_1:focus{background:rgb(231,233,238);color:inherit}._spoiler_1mxys_1:hover a,._spoiler_1mxys_1:focus a{visibility:visible}.dark ._spoiler_1mxys_1:hover,.dark ._spoiler_1mxys_1:focus{background:rgba(255,255,255,.08)}._popover_1ocis_1{z-index:1000}._button_rdrky_1{border:none;background:transparent;padding:0;margin:0}._tooltip_ia50n_1{position:relative;padding:8px;text-align:center;overflow:auto;background:#fff;border-radius:4px;box-shadow:0 0 0 3px #0003;color:#7f919e}._tooltip_ia50n_1{width:300px}._text_ia50n_16{font-size:14px;line-height:1.4;margin:0 0 12px}._tabs_1wiw0_1{display:flex;align-items:stretch}._disabled_1sb5m_1,._selected_1sb5m_1,._tab_1sb5m_1{font-size:12px;display:block;width:100%;padding:10px 18px 10px 10px;overflow:visible;white-space:normal;background:rgba(0,0,0,.03);border:1px solid rgb(235,238,242);border-width:0 1px 1px 0;transition:all .2s ease-in-out;margin-bottom:0;color:#687a86}._disabled_1sb5m_1 input[type=radio],._selected_1sb5m_1 input[type=radio],._tab_1sb5m_1 input[type=radio]{display:none}._disabled_1sb5m_1:first-of-type,._selected_1sb5m_1:first-of-type,._tab_1sb5m_1:first-of-type{border-left:none}._disabled_1sb5m_1:last-of-type,._selected_1sb5m_1:last-of-type,._tab_1sb5m_1:last-of-type{border-right:none}.dark ._disabled_1sb5m_1,.dark ._selected_1sb5m_1,.dark ._tab_1sb5m_1{border-color:#fff3}._tab_1sb5m_1{cursor:pointer}._tab_1sb5m_1:hover{background:rgba(0,0,0,.08)}.dark ._tab_1sb5m_1{color:#fff9;background:rgba(255,255,255,.08)}._selected_1sb5m_1{font-weight:600;color:var(--publisher-color, rgb(46, 159, 255));background:transparent;border-bottom:none}.dark ._selected_1sb5m_1{color:#ffffffd9}._disabled_1sb5m_1{color:#c2c6cc}.dark ._disabled_1sb5m_1{color:#ffffff59;background:rgba(255,255,255,.08)}._text_1sb5m_57{font-size:14px;font-weight:500;display:flex;align-items:center;gap:5px;justify-content:center;margin-bottom:2px;text-align:center}._selected_p9fex_1,._selectable_p9fex_1{position:relative}._selected_p9fex_1:before,._selectable_p9fex_1:before{content:"";display:block;position:absolute;top:-2px;left:-2px;width:calc(100% + 4px);height:calc(100% + 4px);opacity:0;box-shadow:0 0 0 1px #3c4e6e2e;border-radius:4px;transition:opacity ease-in-out .25s}.dark ._selected_p9fex_1:before,.dark ._selectable_p9fex_1:before{box-shadow:0 0 0 1px #fff3}._selectable_p9fex_1:hover:before{opacity:.75}._selected_p9fex_1:before{opacity:1}._delete_p9fex_29{display:block;position:absolute;top:-18px;right:-18px;width:32px;height:32px;padding:6px;color:#fff;background:var(--publisher-color, rgb(46, 159, 255));cursor:pointer;border-radius:50%;box-shadow:0 2px 8px #0003}.light-anchor ._delete_p9fex_29{color:#656c7a}._element_p9fex_47{display:inline;background:inherit}._element_p9fex_47:not(:first-child){margin-top:8px}._image_p9fex_55{display:block;max-width:100%;max-height:20em;box-shadow:none;border-radius:3px}._wrapper_p9fex_63{position:relative;display:block;background:inherit;width:-webkit-fit-content;width:-moz-fit-content;width:fit-content}._loader_p9fex_70{display:block;height:115px;background:rgba(16,48,68,.03);width:150px;border-radius:16px}._dialog_1p5t1_1{background:white;border:2px solid rgba(60,78,110,.18);border-radius:16px}.dark ._dialog_1p5t1_1{background:rgb(42,46,46);border:2px solid rgba(255,255,255,.2)}._submit_1p5t1_11,._cancel_1p5t1_11{border-radius:16px}._form_1p5t1_15{font-size:14px;display:flex;flex-direction:column;gap:8px}._field_1p5t1_22{display:flex;align-items:center;gap:8px}._label_1p5t1_28{color:#7f919e}.dark ._label_1p5t1_28{color:#eee}._input_1p5t1_35{font-size:100%;font-family:inherit;display:block;width:100%;margin:0;padding:4px 6px;white-space:normal;border-radius:4px;border:1px solid rgba(60,78,110,.18);box-sizing:border-box;background:transparent}._input_1p5t1_35:disabled{color:#7f919e}._input_1p5t1_35:focus{border-color:var(--publisher-color, rgb(46, 159, 255));outline:none}._input_1p5t1_35::-webkit-input-placeholder{font-size:13px;color:#7f919e}._input_1p5t1_35::-moz-placeholder{font-size:13px;color:#7f919e}._input_1p5t1_35:-ms-input-placeholder{font-size:13px;color:#7f919e}._input_1p5t1_35::placeholder{font-size:13px;color:#7f919e}.dark ._input_1p5t1_35{color:#eee;border-color:#fff3}.dark ._input_1p5t1_35::-webkit-input-placeholder{color:#ffffff80}.dark ._input_1p5t1_35::-moz-placeholder{color:#ffffff80}.dark ._input_1p5t1_35:-ms-input-placeholder{color:#ffffff80}.dark ._input_1p5t1_35::placeholder{color:#ffffff80}.dark ._input_1p5t1_35:focus{border-color:var(--publisher-color, rgb(46, 159, 255))}._dialog_1p5t1_1{width:300px;padding:16px;border-width:1px;box-shadow:0 4px 8px -1px #0000001a}.dark ._dialog_1p5t1_1{border-width:1px}._buttons_1p5t1_80{display:flex;width:100%;align-items:center;justify-content:flex-end;gap:8px}._cancel_1p5t1_11{background:none;box-shadow:0 0 0 1px #c2c6cc;color:#7f919e}._cancel_1p5t1_11:hover{color:#7f919e;background:rgb(231,233,238)}.dark ._cancel_1p5t1_11{color:#eee;box-shadow:0 0 0 1px #fff3}.dark ._cancel_1p5t1_11:hover{background:rgb(73,78,88)}._selected_10kig_1,._selectable_10kig_1{position:relative}._selected_10kig_1:before,._selectable_10kig_1:before{content:"";display:block;position:absolute;top:-2px;left:-2px;width:calc(100% + 4px);height:calc(100% + 4px);opacity:0;box-shadow:0 0 0 1px #3c4e6e2e;border-radius:4px;transition:opacity ease-in-out .25s}.dark ._selected_10kig_1:before,.dark ._selectable_10kig_1:before{box-shadow:0 0 0 1px #fff3}._selectable_10kig_1:hover:before{opacity:.75}._selected_10kig_1:before{opacity:1}._delete_10kig_29{display:block;position:absolute;top:-18px;right:-18px;width:32px;height:32px;padding:6px;color:#fff;background:var(--publisher-color, rgb(46, 159, 255));cursor:pointer;border-radius:50%;box-shadow:0 2px 8px #0003}.light-anchor ._delete_10kig_29{color:#656c7a}._preview_10kig_47{display:block;position:relative;width:-webkit-fit-content;width:-moz-fit-content;width:fit-content;min-width:300px;max-width:100%;border-radius:3px}._no-pointer_10kig_56{display:block;overflow:hidden;pointer-events:none;border-radius:3px}._button_1ka0a_1,._button_1ka0a_1:active,._button_1ka0a_1:hover{flex:0 0 auto;background:none;padding:0;border:none;cursor:pointer;margin:0;width:24px;height:24px;display:flex;align-items:center;justify-content:center;color:#7f919e;opacity:.6;border-radius:4px}._button_1ka0a_1:hover{opacity:1}._button_1ka0a_1:disabled{opacity:.25;cursor:default}.dark ._button_1ka0a_1{color:#ffffff80}._button_1ka0a_1:active,._button_1ka0a_1:hover{opacity:1;background:rgba(60,75,120,.12)}.dark ._button_1ka0a_1:active,.dark ._button_1ka0a_1:hover{background:rgba(255,255,255,.2)}._container_1ka0a_36{font-size:14px;display:flex;align-items:center;gap:8px;position:relative;overflow-y:auto;max-height:400px;max-width:90vw;padding:5px 12px;border:1px solid rgb(231,233,238);border-radius:16px;background:#fff;box-shadow:0 4px 8px -1px #0000001a}.dark ._container_1ka0a_36{border:1px solid rgba(255,255,255,.2);background:rgb(42,46,46)}._link_1ka0a_56{color:var(--publisher-color, rgb(46, 159, 255));max-width:300px;overflow:hidden;white-space:nowrap;text-overflow:ellipsis;outline:none}._button_1ka0a_1{font-size:14px;opacity:1}._separator_1ka0a_69{display:inline-block;width:1px;height:18px;background:rgb(194,198,204)}.dark ._separator_1ka0a_69{background:rgba(255,255,255,.35)}._icon_1ka0a_79{width:16px;height:16px}._block_865k6_1{display:block;max-width:-webkit-max-content;max-width:-moz-max-content;max-width:max-content}._selected_12oyo_1,._mention_12oyo_1{font-weight:700;position:relative}._selected_12oyo_1:before,._mention_12oyo_1:before{content:"@"}._selected_12oyo_1{-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;border-radius:2px;background:rgba(60,78,110,.18);box-shadow:0 0 0 2px #3c4e6e2e}.dark ._selected_12oyo_1{box-shadow:0 0 0 2px #fff3;background:rgba(255,255,255,.2)}._default_1alzx_1{background:rgb(127,145,158);color:transparent}._default_1alzx_1 a,._default_1alzx_1 img,._default_1alzx_1 video{visibility:hidden}.dark ._default_1alzx_1{background:rgba(255,255,255,.5)}._active_1alzx_12,._default_1alzx_1:hover{background:rgb(231,233,238);color:inherit}._active_1alzx_12 a,._default_1alzx_1:hover a,._active_1alzx_12 img,._default_1alzx_1:hover img,._active_1alzx_12 video,._default_1alzx_1:hover video{visibility:visible}.dark ._active_1alzx_12,.dark ._default_1alzx_1:hover{background:rgba(255,255,255,.2)}._selected_1lt3i_1,._selectable_1lt3i_1{position:relative}._selected_1lt3i_1:before,._selectable_1lt3i_1:before{content:"";display:block;position:absolute;top:-2px;left:-2px;width:calc(100% + 4px);height:calc(100% + 4px);opacity:0;box-shadow:0 0 0 1px #3c4e6e2e;border-radius:4px;transition:opacity ease-in-out .25s}.dark ._selected_1lt3i_1:before,.dark ._selectable_1lt3i_1:before{box-shadow:0 0 0 1px #fff3}._selectable_1lt3i_1:hover:before{opacity:.75}._selected_1lt3i_1:before{opacity:1}._delete_1lt3i_29{display:block;position:absolute;top:-18px;right:-18px;width:32px;height:32px;padding:6px;color:#fff;background:var(--publisher-color, rgb(46, 159, 255));cursor:pointer;border-radius:50%;box-shadow:0 2px 8px #0003}.light-anchor ._delete_1lt3i_29{color:#656c7a}._video_1lt3i_47{display:block;position:relative;max-width:100%;max-height:20em;box-shadow:none;border-radius:3px;pointer-events:none}._container_1lt3i_57{display:inline;position:relative;background:inherit}._wrapper_1lt3i_63{position:relative;display:block;background:inherit;width:-webkit-fit-content;width:-moz-fit-content;width:fit-content}._quote_11ucn_1{margin:8px 0 0;padding:0 0 0 12px;border-left:4px solid #687a86}._quote_11ucn_1:first-child{margin-top:0}._code_vr1vh_1{font-family:monospace}._button_1559b_1,._active_1559b_1{flex:0 0 auto;background:none;padding:0;border:none;cursor:pointer;margin:0;width:24px;height:24px;display:flex;align-items:center;justify-content:center;color:#7f919e;opacity:.6;border-radius:4px}._button_1559b_1:hover,._active_1559b_1:hover{opacity:1}._button_1559b_1:disabled,._active_1559b_1:disabled{opacity:.25;cursor:default}.dark ._button_1559b_1,.dark ._active_1559b_1{color:#ffffff80}._active_1559b_1{opacity:1;background:rgba(60,75,120,.12)}.dark ._active_1559b_1{background:rgba(255,255,255,.2)}._icon_1559b_36{width:16px;height:16px}._button_12m5a_1{flex:0 0 auto;background:none;padding:0;border:none;cursor:pointer;margin:0;width:24px;height:24px;display:flex;align-items:center;justify-content:center;color:#7f919e;opacity:.6;border-radius:4px}._button_12m5a_1:hover{opacity:1}._button_12m5a_1:disabled{opacity:.25;cursor:default}.dark ._button_12m5a_1{color:#ffffff80}._categories_19ro8_1{display:flex;flex-direction:column;gap:2px;max-height:400px;padding:4px 10px;overflow-y:scroll}@media only screen and (min-width: 481px){._categories_19ro8_1{flex-direction:row}}@media only screen and (min-width: 768px){._categories_19ro8_1{max-height:800px}}._column_19ro8_20{display:flex;flex-direction:column;gap:2px}._image_19ro8_26{display:flex;align-items:center;justify-content:center;position:relative;cursor:pointer;border:2px solid white;background:rgb(235,238,242)}._image_19ro8_26:hover{border-color:#2e87e7;border-radius:2px}._image_19ro8_26:before{content:"";display:block;position:absolute;top:0;left:0;width:100%;height:100%;opacity:.6;background:#2a2e2e}.dark ._image_19ro8_26{background:rgba(255,255,255,.2);border-color:#2a2e2e}.dark ._image_19ro8_26:hover{border-color:#2e87e7}._title_19ro8_58{display:flex;align-items:center;justify-content:center;position:absolute;top:0;left:0;width:100%;height:100%;color:#fff;text-shadow:1px 1px #2a2e2e;text-align:center}._gifs_lkt4v_1{max-height:400px;padding:4px 10px;overflow-y:scroll}@media only screen and (min-width: 481px){._gifs_lkt4v_1{max-height:800px}}._columns_lkt4v_12{display:flex;flex-direction:column;gap:2px}@media only screen and (min-width: 481px){._columns_lkt4v_12{flex-direction:row}}._column_lkt4v_12{display:flex;flex-direction:column;gap:2px}._image_lkt4v_29{display:flex;align-items:center;justify-content:center;position:relative;max-width:100%;cursor:pointer;border:2px solid white;background:rgb(235,238,242)}._image_lkt4v_29:hover{border-color:#2e9fff;border-radius:2px}.dark ._image_lkt4v_29{background:rgba(255,255,255,.2);border-color:#2a2e2e}.dark ._image_lkt4v_29:hover{border-color:#2e87e7}._searchbox_1ftd7_1{box-sizing:border-box;padding:12px 10px 8px}._input_1ftd7_6{font-size:100%;font-family:inherit;display:block;width:100%;min-height:28px;height:auto;margin:0;padding:2px 6px;white-space:normal;border-radius:4px;border:2px solid rgba(60,78,110,.18);box-sizing:border-box;outline:none}.dark ._input_1ftd7_6{color:#eee;border-color:#ffffff80;background:transparent}@-webkit-keyframes _pulse_drbub_1{0%{opacity:.5}50%{opacity:1}to{opacity:.5}}@keyframes _pulse_drbub_1{0%{opacity:.5}50%{opacity:1}to{opacity:.5}}._skeleton_drbub_12{display:flex;flex-direction:column;gap:6px;padding:4px 12px;max-height:400px;overflow:hidden}@media only screen and (min-width: 481px){._skeleton_drbub_12{flex-direction:row;max-height:800px}}._image_drbub_27{background:rgb(235,238,242);-webkit-animation:_pulse_drbub_1 2s ease-in-out infinite;animation:_pulse_drbub_1 2s ease-in-out infinite}.dark ._image_drbub_27{background:rgba(255,255,255,.2)}._column_drbub_35{display:flex;flex-direction:column;gap:6px}._root_lwhkh_1{position:relative}._popout_lwhkh_5{display:flex;flex-direction:column;justify-content:space-between;align-items:center;max-height:98vh;background:white;border-radius:4px;border:2px solid rgba(60,78,110,.18)}.dark ._popout_lwhkh_5{border-color:#fff3;background:rgb(42,46,46)}._searchbox_lwhkh_20{width:100%}._powered-by_lwhkh_24{width:100px;margin:4px 12px;color:#748793}.dark ._powered-by_lwhkh_24{color:#ffffff59}._zone_1xgo0_1{border-radius:16px;position:absolute;inset:0;background-color:#0006;z-index:1;display:flex;align-items:center;justify-content:center}._wrapper_1xgo0_12{border:3px dashed #fff;padding:12px;width:90%;text-align:center;border-radius:6px}._text_1xgo0_20{color:#fff;font-size:15px;font-weight:700;font-style:normal}._placeholder_s9avi_1{font-style:normal;font-weight:400;position:absolute;top:20px;left:20px;color:#000;opacity:.33;line-height:1.4}.dark ._placeholder_s9avi_1{color:#fff}._separator_p7fwl_1{display:inline-block;width:2px;height:24px;margin:0 2px;background:rgba(60,78,110,.18)}.dark ._separator_p7fwl_1{background:rgba(255,255,255,.08)}._expand_k0g7a_1,._expand-active_k0g7a_1{flex:0 0 auto;background:none;padding:0;border:none;cursor:pointer;margin:0;width:24px;height:24px;display:flex;align-items:center;justify-content:center;color:#7f919e;opacity:.6;border-radius:4px}._expand_k0g7a_1:hover,._expand-active_k0g7a_1:hover{opacity:1}._expand_k0g7a_1:disabled,._expand-active_k0g7a_1:disabled{opacity:.25;cursor:default}.dark ._expand_k0g7a_1,.dark ._expand-active_k0g7a_1{color:#ffffff80}._expand-active_k0g7a_1{opacity:1;background:rgba(60,75,120,.12)}.dark ._expand-active_k0g7a_1{background:rgba(255,255,255,.2)}._expand-active_k0g7a_1,._expand_k0g7a_1{width:16px;height:16px}._menu_k0g7a_41{display:inline-flex;align-items:center;gap:4px}._toolbar_k0g7a_47{padding:5px 6px}._toolbar-primary_k0g7a_51{display:flex;justify-content:space-between}._toolbar-secondary-multirow_k0g7a_56,._toolbar-secondary_k0g7a_56{display:grid;grid-auto-flow:column;gap:6px;margin-top:6px}._toolbar-secondary-multirow_k0g7a_56{grid-template-rows:repeat(2,auto)}._expand_k0g7a_1{font-size:14px}._expand_k0g7a_1:hover{opacity:.6}._expand-active_k0g7a_1{background:none}._actions_k0g7a_78{display:inline-flex;gap:12px;margin-right:-5px}._submit_k0g7a_84{font-size:15px;font-style:normal;font-weight:700;position:relative;line-height:1;height:-webkit-fit-content;height:-moz-fit-content;height:fit-content;padding:7px 15px;border:none;text-shadow:none;white-space:nowrap;border-radius:14px;color:#fff;background-color:var(--publisher-color, rgb(46, 159, 255))}.light-anchor ._submit_k0g7a_84{color:#656c7a}._submit_k0g7a_84:before{content:"";display:block;position:absolute;top:0;left:0;width:100%;height:100%;background:rgba(0,0,0,.1);border-radius:14px;opacity:0;transition:all .25s ease-in-out}._submit_k0g7a_84:hover{background-color:var(--publisher-color, rgb(46, 159, 255))}._submit_k0g7a_84:hover:before{opacity:1}._submit-text_k0g7a_122{position:relative}._cancel_k0g7a_126{padding:7px;background-color:transparent;border:none;color:var(--publisher-color, rgb(46, 159, 255));cursor:pointer;font-weight:700;font-size:15px;line-height:1}._cancel_k0g7a_126:hover{color:#656c7a}._button_1wqlf_1{flex:0 0 auto;background:none;padding:0;border:none;cursor:pointer;margin:0;width:24px;height:24px;display:flex;align-items:center;justify-content:center;color:#7f919e;opacity:.6;border-radius:4px}._button_1wqlf_1:hover{opacity:1}._button_1wqlf_1:disabled{opacity:.25;cursor:default}.dark ._button_1wqlf_1{color:#ffffff80}._icon_1wqlf_28{width:16px;height:16px}._container_if5tp_1{background:white;border:2px solid rgba(60,78,110,.18);border-radius:16px}.dark ._container_if5tp_1{background:rgb(42,46,46);border:2px solid rgba(255,255,255,.2)}._item-active_if5tp_11,._item_if5tp_11{font-weight:500;display:flex;gap:16px;position:relative;padding:8px;overflow:hidden;cursor:pointer;color:#7f919e}.dark ._item-active_if5tp_11,.dark ._item_if5tp_11{color:#ffffff80}._container_if5tp_1{width:100%;max-height:200px;padding:0;overflow-y:scroll;border-radius:0 0 3px 3px}._header_if5tp_33{font-size:11px;font-weight:700;text-transform:uppercase;line-height:11px;margin:0;padding:8px;color:#7f919e;border:0}.dark ._header_if5tp_33{color:#ffffff80}._list_if5tp_47{margin:0;padding:0;list-style:none}._item_if5tp_11{background:transparent}._item_if5tp_11:hover{background:rgb(247,249,250)}.dark ._item_if5tp_11:hover{background:rgba(255,255,255,.08)}._item-active_if5tp_11{background:rgb(46,159,255);color:#fff}.dark ._item-active_if5tp_11{color:#fff}._avatar_if5tp_71{flex:0 0 auto;width:22px;height:22px}._overlay_1i4qh_1{z-index:1000}._container_ylcfx_1{position:relative;background:#fff;border:2px solid rgba(60,78,110,.18);border-radius:16px;line-height:1.4}.dark ._container_ylcfx_1{background:rgba(255,255,255,.05);border:2px solid rgba(255,255,255,.2)}._editor-expanded_ylcfx_13,._editor_ylcfx_13{position:relative;overflow-y:auto;max-height:350px;min-height:65px!important;padding:20px;transition:all .15s ease-in-out;outline:none;color:#2a2e2e;word-break:break-word}._editor-expanded_ylcfx_13 p,._editor_ylcfx_13 p{margin:8px 0 0}._editor-expanded_ylcfx_13 p:first-child,._editor_ylcfx_13 p:first-child{margin-top:0}._editor-expanded_ylcfx_13 a:link,._editor_ylcfx_13 a:link,._editor-expanded_ylcfx_13 a:visited,._editor_ylcfx_13 a:visited{color:var(--publisher-color, rgb(46, 159, 255))}.dark ._editor-expanded_ylcfx_13,.dark ._editor_ylcfx_13{color:#eee}._editor-container-expanded_ylcfx_37,._editor-container_ylcfx_37{overflow:hidden;border-radius:16px}._editor-container-expanded_ylcfx_37{border-radius:16px 16px 0 0}._editor-expanded_ylcfx_13{border-bottom:2px solid rgba(60,78,110,.18);border-radius:16px 16px 0 0;min-height:115px!important}.dark ._editor-expanded_ylcfx_13{border-bottom:2px solid rgba(255,255,255,.2)}._placeholder-submit-button_ylcfx_55{display:none}
</style><link rel="stylesheet" href="./lounge.2947a98de15e03c64b9251e985a1725e.css"><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="lounge/main" src="./lounge.bundle.410b7616327eac2412b39d8d8f95e898.js.다운로드"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="remote/config" src="./config.js.다운로드"></script><style id="css_1732752053322"></style><!--<base target="_parent">--><base href="." target="_parent"><style type="text/css">@import url("https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,400;0,500;0,600;0,700;1,400;1,700&display=swap"); body.roboto , body.roboto input, body.roboto select, body.roboto textarea { font-family: Roboto, sans-serif; }</style><link rel="stylesheet" href="./highlight.3128dd90ecaebd8542ac3442033f3f00.css"><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="fb" src="./sdk(1).js.다운로드"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="gapi" src="./api.js.다운로드" gapi_processed="true"></script><script type="text/javascript" charset="utf-8" async="" data-requirecontext="_" data-requiremodule="common/vendor_extensions/highlight" src="./highlight.6fbf348532f299e045c254c49c4dbedf.js.다운로드"></script><style type="text/css" data-fbcssmodules="css:fb.css.base css:fb.css.dialog css:fb.css.iframewidget">.fb_hidden{position:absolute;top:-10000px;z-index:10001}.fb_reposition{overflow:hidden;position:relative}.fb_invisible{display:none}.fb_reset{background:none;border:0px;border-spacing:0;color:#000;cursor:auto;direction:ltr;font-family:'lucida grande', tahoma, verdana, arial, sans-serif;font-size:11px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:1;margin:0;overflow:visible;padding:0;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;visibility:visible;white-space:normal;word-spacing:normal}.fb_reset>div{overflow:hidden}@keyframes fb_transform{from{opacity:0;transform:scale(.95)}to{opacity:1;transform:scale(1)}}.fb_animate{animation:fb_transform .3s forwards}
.fb_hidden{position:absolute;top:-10000px;z-index:10001}.fb_reposition{overflow:hidden;position:relative}.fb_invisible{display:none}.fb_reset{background:none;border:0px;border-spacing:0;color:#000;cursor:auto;direction:ltr;font-family:'lucida grande', tahoma, verdana, arial, sans-serif;font-size:11px;font-style:normal;font-variant:normal;font-weight:normal;letter-spacing:normal;line-height:1;margin:0;overflow:visible;padding:0;text-align:left;text-decoration:none;text-indent:0;text-shadow:none;text-transform:none;visibility:visible;white-space:normal;word-spacing:normal}.fb_reset>div{overflow:hidden}@keyframes fb_transform{from{opacity:0;transform:scale(.95)}to{opacity:1;transform:scale(1)}}.fb_animate{animation:fb_transform .3s forwards}
.fb_dialog{background:rgba(82, 82, 82, .7);position:absolute;top:-10000px;z-index:10001}.fb_dialog_advanced{border-radius:8px;padding:10px}.fb_dialog_content{background:#fff;color:#373737}.fb_dialog_close_icon{background:url(https://connect.facebook.net/rsrc.php/v3/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 0 transparent;cursor:pointer;display:block;height:15px;position:absolute;right:18px;top:17px;width:15px}.fb_dialog_mobile .fb_dialog_close_icon{left:5px;right:auto;top:5px}.fb_dialog_padding{background-color:transparent;position:absolute;width:1px;z-index:-1}.fb_dialog_close_icon:hover{background:url(https://connect.facebook.net/rsrc.php/v3/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 -15px transparent}.fb_dialog_close_icon:active{background:url(https://connect.facebook.net/rsrc.php/v3/yq/r/IE9JII6Z1Ys.png) no-repeat scroll 0 -30px transparent}.fb_dialog_iframe{line-height:0}.fb_dialog_content .dialog_title{background:#6d84b4;border:1px solid #365899;color:#fff;font-size:14px;font-weight:bold;margin:0}.fb_dialog_content .dialog_title>span{background:url(https://connect.facebook.net/rsrc.php/v3/yd/r/Cou7n-nqK52.gif) no-repeat 5px 50%;float:left;padding:5px 0 7px 26px}body.fb_hidden{height:100%;left:0px;margin:0px;overflow:visible;position:absolute;top:-10000px;transform:none;width:100%}.fb_dialog.fb_dialog_mobile.loading{background:url(https://connect.facebook.net/rsrc.php/v3/ya/r/3rhSv5V8j3o.gif) white no-repeat 50% 50%;min-height:100%;min-width:100%;overflow:hidden;position:absolute;top:0;z-index:10001}.fb_dialog.fb_dialog_mobile.loading.centered{background:none;height:auto;min-height:initial;min-width:initial;width:auto}.fb_dialog.fb_dialog_mobile.loading.centered #fb_dialog_loader_spinner{width:100%}.fb_dialog.fb_dialog_mobile.loading.centered .fb_dialog_content{background:none}.loading.centered #fb_dialog_loader_close{clear:both;color:#fff;display:block;font-size:18px;padding-top:20px}#fb-root #fb_dialog_ipad_overlay{background:rgba(0, 0, 0, .4);bottom:0;left:0;min-height:100%;position:absolute;right:0;top:0;width:100%;z-index:10000}#fb-root #fb_dialog_ipad_overlay.hidden{display:none}.fb_dialog.fb_dialog_mobile.loading iframe{visibility:hidden}.fb_dialog_mobile .fb_dialog_iframe{position:sticky;top:0}.fb_dialog_content .dialog_header{background:linear-gradient(from(#738aba), to(#2c4987));border-bottom:1px solid;border-color:#043b87;box-shadow:white 0px 1px 1px -1px inset;color:#fff;font:bold 14px Helvetica, sans-serif;text-overflow:ellipsis;text-shadow:rgba(0, 30, 84, .296875) 0px -1px 0px;vertical-align:middle;white-space:nowrap}.fb_dialog_content .dialog_header table{height:43px;width:100%}.fb_dialog_content .dialog_header td.header_left{font-size:12px;padding-left:5px;vertical-align:middle;width:60px}.fb_dialog_content .dialog_header td.header_right{font-size:12px;padding-right:5px;vertical-align:middle;width:60px}.fb_dialog_content .touchable_button{background:linear-gradient(from(#4267B2), to(#2a4887));background-clip:padding-box;border:1px solid #29487d;border-radius:3px;display:inline-block;line-height:18px;margin-top:3px;max-width:85px;padding:4px 12px;position:relative}.fb_dialog_content .dialog_header .touchable_button input{background:none;border:none;color:#fff;font:bold 12px Helvetica, sans-serif;margin:2px -12px;padding:2px 6px 3px 6px;text-shadow:rgba(0, 30, 84, .296875) 0px -1px 0px}.fb_dialog_content .dialog_header .header_center{color:#fff;font-size:16px;font-weight:bold;line-height:18px;text-align:center;vertical-align:middle}.fb_dialog_content .dialog_content{background:url(https://connect.facebook.net/rsrc.php/v3/y9/r/jKEcVPZFk-2.gif) no-repeat 50% 50%;border:1px solid #4a4a4a;border-bottom:0;border-top:0;height:150px}.fb_dialog_content .dialog_footer{background:#f5f6f7;border:1px solid #4a4a4a;border-top-color:#ccc;height:40px}#fb_dialog_loader_close{float:left}.fb_dialog.fb_dialog_mobile .fb_dialog_close_icon{visibility:hidden}#fb_dialog_loader_spinner{animation:rotateSpinner 1.2s linear infinite;background-color:transparent;background-image:url(https://connect.facebook.net/rsrc.php/v3/yD/r/t-wz8gw1xG1.png);background-position:50% 50%;background-repeat:no-repeat;height:24px;width:24px}@keyframes rotateSpinner{0%{transform:rotate(0deg)}100%{transform:rotate(360deg)}}
.fb_iframe_widget{display:inline-block;position:relative}.fb_iframe_widget span{display:inline-block;position:relative;text-align:justify}.fb_iframe_widget iframe{position:absolute}.fb_iframe_widget_fluid_desktop,.fb_iframe_widget_fluid_desktop span,.fb_iframe_widget_fluid_desktop iframe{max-width:100%}.fb_iframe_widget_fluid_desktop iframe{min-width:220px;position:relative}.fb_iframe_widget_lift{z-index:1}.fb_iframe_widget_fluid{display:inline}.fb_iframe_widget_fluid span{width:100%}</style></head>
<body class="roboto dark-anchor">
    

    
    <div id="error" class="alert--error">
        <p>We were unable to load Disqus. If you are a moderator please see our <a href="https://docs.disqus.com/help/83/"> troubleshooting guide</a>. </p>
    </div>

    
    <script type="text/json" id="disqus-forumData">{"session":{"canModerate":false,"audienceSyncVerified":false,"canReply":true,"mustVerify":false,"recaptchaPublicKey":"6Lfx6u0SAAAAAI1QkeTW397iQv1MsBfbDaYlwxK_","mustVerifyEmail":false},"forum":{"aetBannerConfirmation":null,"founder":"213053281","twitterName":null,"commentsLinkOne":"1 Comment","guidelines":null,"disableDisqusBrandingOnPolls":false,"commentsLinkZero":"0 Comments","disableDisqusBranding":false,"id":"harvard-nlp","createdAt":"2018-04-04T12:32:53.273776","category":"Tech","aetBannerEnabled":false,"aetBannerTitle":null,"raw_guidelines":null,"initialCommentCount":null,"votingType":null,"daysUnapproveNewUsers":null,"installCompleted":false,"moderatorBadgeText":"","commentPolicyText":null,"aetEnabled":false,"channel":null,"sort":4,"description":null,"newPolicy":true,"raw_description":null,"customFont":null,"language":"en","adsReviewStatus":1,"commentsPlaceholderTextEmpty":null,"daysAlive":0,"forumCategory":{"date_added":"2016-01-28T01:54:31","id":8,"name":"Tech"},"linkColor":null,"colorScheme":"auto","pk":"5460729","commentsPlaceholderTextPopulated":null,"permissions":{},"commentPolicyLink":null,"aetBannerDescription":null,"favicon":{"permalink":"https://disqus.com/api/forums/favicons/harvard-nlp.jpg","cache":"//a.disquscdn.com/1730296160/images/favicon-default.png"},"name":"Harvard NLP","commentsLinkMultiple":"{num} Comments","settings":{"threadRatingsEnabled":false,"adsDRNativeEnabled":false,"behindClickEnabled":false,"disable3rdPartyTrackers":false,"adsVideoEnabled":false,"adsProductVideoEnabled":false,"adsPositionPollEnabled":false,"adsPositionTopEnabled":false,"ssoRequired":false,"unapproveLinks":false,"adsPositionRecommendationsEnabled":false,"linkAffiliationEnabled":true,"adsProductLinksThumbnailsEnabled":false,"adsProductStoriesEnabled":false,"organicDiscoveryEnabled":true,"adsProductDisplayEnabled":false,"adsProductLinksEnabled":false,"audienceSyncEnabled":false,"threadReactionsEnabled":false,"adsEnabled":false,"disableSocialShare":false,"allowAnonPost":false,"hasCustomAvatar":false,"sidebarEnabled":false,"adultContent":false,"allowAnonVotes":false,"gifPickerEnabled":true,"mustVerify":true,"badgesEnabled":false,"mustVerifyEmail":true,"unapproveNewUsersEnabled":false,"mediaembedEnabled":true,"userIdentityDisabled":false,"adsPositionBottomEnabled":false,"discoveryLocked":false,"validateAllPosts":false,"adsSettingsLocked":false,"isVIP":false,"adsPositionInthreadEnabled":false},"organizationId":4104443,"typeface":"auto","url":"http://nlp.seas.harvard.edu","daysThreadAlive":0,"avatar":{"small":{"permalink":"https://disqus.com/api/forums/avatars/harvard-nlp.jpg?size=32","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"large":{"permalink":"https://disqus.com/api/forums/avatars/harvard-nlp.jpg?size=92","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"}},"signedUrl":"http://disq.us/?url=http%3A%2F%2Fnlp.seas.harvard.edu&key=7UAWL2vyUsMpGDRuYdM5yw"}}</script>

    <script type="text/json" id="disqus-threadData">{"cursor":{"hasPrev":false,"prev":null,"total":138,"hasNext":true,"next":"1:0:0"},"code":0,"response":{"lastModified":1730790136,"posts":[{"editableUntil":"2018-11-10T22:56:30","dislikes":1,"thread":"6595120245","numReports":0,"likes":10,"message":"\u003cp>Thank you for a great piece.\u003c/p>\u003cp>I have a question about the positional encoding.  Here you seem to interleave sine and cosine curves over the dimensions for a given position.  The indexing in the original paper suggests the same, but the code at \u003ca href=\"https://disq.us/url?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensor2tensor%2Fblob%2Fmaster%2Ftensor2tensor%2Flayers%2Fcommon_attention.py%3ACTNdLeTn1cJXk0PU4IiBnjHRR2A&amp;cuid=5460729\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py\">https://github.com/tensorfl...\u003c/a> (and the discussion at \u003ca href=\"http://disq.us/url?url=http%3A%2F%2Fjalammar.github.io%2Fillustrated-transformer%2F%3AJ0pFreh0LdTMQXRXyIcFL5TyZUs&amp;cuid=5460729\" rel=\"nofollow noopener\" target=\"_blank\" title=\"http://jalammar.github.io/illustrated-transformer/\">http://jalammar.github.io/i...\u003c/a> ) suggests a concatenation of sines on the left and cosines on the right.\u003c/p>\u003cp>I'm not sure it makes a substantive difference, but would you agree there's a difference in implementation here?\u003c/p>","id":"4177145673","createdAt":"2018-11-03T21:56:30","author":{"username":"free_variation","about":"","name":"free_variation","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2013-06-06T03:35:56","profileUrl":"https://disqus.com/by/free_variation/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"54958964","avatar":{"permalink":"https://disqus.com/api/users/avatars/free_variation.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/free_variation.jpg","cache":"https://c.disquscdn.com/uploads/users/5495/8964/avatar200.jpg?1650090694"},"cache":"https://c.disquscdn.com/uploads/users/5495/8964/avatar92.jpg?1650090694","large":{"permalink":"https://disqus.com/api/users/avatars/free_variation.jpg","cache":"https://c.disquscdn.com/uploads/users/5495/8964/avatar92.jpg?1650090694"},"small":{"permalink":"https://disqus.com/api/users/avatars/free_variation.jpg","cache":"https://c.disquscdn.com/uploads/users/5495/8964/avatar32.jpg?1650090694"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Thank you for a great piece.\n\nI have a question about the positional encoding.  Here you seem to interleave sine and cosine curves over the dimensions for a given position.  The indexing in the original paper suggests the same, but the code at https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py (and the discussion at http://jalammar.github.io/illustrated-transformer/ ) suggests a concatenation of sines on the left and cosines on the right.\n\nI'm not sure it makes a substantive difference, but would you agree there's a difference in implementation here?","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":0,"points":9,"moderationLabels":["links"],"isEdited":false,"sb":false},{"editableUntil":"2020-07-22T14:47:12","dislikes":0,"thread":"6595120245","numReports":0,"likes":0,"message":"\u003cp>free_variation did you ever find the answer to this?\u003c/p>","id":"4992836748","createdAt":"2020-07-15T14:47:12","author":{"username":"disqus_vsvfcRoRBq","about":"","name":"Paul Nguyen","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2020-07-10T13:23:24","profileUrl":"https://disqus.com/by/disqus_vsvfcRoRBq/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"352815913","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_vsvfcRoRBq.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_vsvfcRoRBq.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_vsvfcRoRBq.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_vsvfcRoRBq.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4177145673,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"free_variation did you ever find the answer to this?","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2022-10-10T13:06:06","dislikes":0,"thread":"6595120245","numReports":0,"likes":0,"message":"\u003cp>Yes. This is easier to implement just concatenating them instead of interleaving them. It makes no difference unless you're trying to do something like load a checkpoint from another implementation.\u003c/p>","id":"6002455660","createdAt":"2022-10-03T13:06:06","author":{"username":"mark_daoust","about":"","name":"Mark Daoust","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-07-28T17:37:19","profileUrl":"https://disqus.com/by/mark_daoust/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"260458999","avatar":{"permalink":"https://disqus.com/api/users/avatars/mark_daoust.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/mark_daoust.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/mark_daoust.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/mark_daoust.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4992836748,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Yes. This is easier to implement just concatenating them instead of interleaving them. It makes no difference unless you're trying to do something like load a checkpoint from another implementation.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":2,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2020-04-29T21:38:37","dislikes":0,"thread":"6595120245","numReports":0,"likes":5,"message":"\u003cp>The loss computation appears to be updating gradients during validation. \u003cbr>This appears to drive weight updates to fit the validation (test) data, which it should not.\u003c/p>\u003cp>A recommended way to run validation is apparently: \u003cbr>\u003ccode>model.eval()\u003cbr>torch.no_grad()\u003c/code>\u003cbr>[\u003ca href=\"https://disq.us/url?url=https%3A%2F%2Fdiscuss.pytorch.org%2Ft%2Fmodel-eval-vs-with-torch-no-grad%2F19615%3Ae_08I4EBqohG3_nIbsY1i9V_SvA&amp;cuid=5460729\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615\">https://discuss.pytorch.org...\u003c/a>](url)\u003cbr>\u003ca href=\"https://disq.us/url?url=https%3A%2F%2Fdiscuss.pytorch.org%2Ft%2Ftwo-small-questions-about-with-torch-no-grad%2F27571%3AzewGg_fpEMy6QbmM9X4Lh6ovyRk&amp;cuid=5460729\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://discuss.pytorch.org/t/two-small-questions-about-with-torch-no-grad/27571\">https://discuss.pytorch.org...\u003c/a>\u003c/p>\u003cp>And indeed, the code that runs without error is the following, where loss.backward()\u003cbr>is pushed under the if statement:\u003c/p>\u003cp>\u003c/p>\u003cpre>\u003ccode>        \u003cbr>        model.eval()\u003cbr>        with torch.no_grad():\u003cbr>            test_loss = run_epoch(val_dataloader, model, \u003cbr>                  SimpleLossCompute(model.generator, criterion, None))\u003cbr>            print(\"test_loss\", test_loss)\u003cbr>\u003c/code>\u003c/pre>\u003cp>\u003cbr>and: \u003cbr>\u003c/p>\u003cpre>\u003ccode>\u003cbr>class SimpleLossCompute:\u003cbr>    \"A simple loss compute and train function.\"\u003cbr>    def __init__(self, generator, criterion, opt=None):\u003cbr>        self.generator = generator\u003cbr>        self.criterion = criterion\u003cbr>        self.opt = opt\u003cbr>        \u003cbr>    def __call__(self, x, y, norm):\u003cbr>        x = self.generator(x)\u003cbr>        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \u003cbr>                              y.contiguous().view(-1)) / norm\u003cbr>        #loss.backward()\u003cbr>        if self.opt is not None:\u003cbr>            loss.backward()\u003cbr>            self.opt.step()\u003cbr>            self.opt.optimizer.zero_grad()\u003cbr>        return loss.data * norm\u003c/code>\u003c/pre>\u003cp>\u003c/p>\u003cp>This was discovered in another forum.\u003c/p>","id":"4885888969","createdAt":"2020-04-22T21:38:37","author":{"username":"disqus_wGGAF5yQKM","about":"","name":"Gus Smith","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-10-05T17:03:42","profileUrl":"https://disqus.com/by/disqus_wGGAF5yQKM/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"177403162","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_wGGAF5yQKM.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_wGGAF5yQKM.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_wGGAF5yQKM.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_wGGAF5yQKM.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"The loss computation appears to be updating gradients during validation. \nThis appears to drive weight updates to fit the validation (test) data, which it should not. \n\nA recommended way to run validation is apparently: \n\u003ccode>model.eval()\ntorch.no_grad()\u003c/code>\n[https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615](url)\nhttps://discuss.pytorch.org/t/two-small-questions-about-with-torch-no-grad/27571\n\nAnd indeed, the code that runs without error is the following, where loss.backward()\nis pushed under the if statement: \n\n\u003cpre>\n\u003ccode>        \n        model.eval()\n        with torch.no_grad():\n            test_loss = run_epoch(val_dataloader, model, \n                  SimpleLossCompute(model.generator, criterion, None))\n            print(\"test_loss\", test_loss)\n\u003c/code>\u003c/pre>\nand: \n\u003cpre>\u003ccode>\nclass SimpleLossCompute:\n    \"A simple loss compute and train function.\"\n    def __init__(self, generator, criterion, opt=None):\n        self.generator = generator\n        self.criterion = criterion\n        self.opt = opt\n        \n    def __call__(self, x, y, norm):\n        x = self.generator(x)\n        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n                              y.contiguous().view(-1)) / norm\n        #loss.backward()\n        if self.opt is not None:\n            loss.backward()\n            self.opt.step()\n            self.opt.optimizer.zero_grad()\n        return loss.data * norm\u003c/code>\u003c/pre>\n\nThis was discovered in another forum.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":0,"points":5,"moderationLabels":["links"],"isEdited":true,"sb":false},{"editableUntil":"2020-07-17T13:25:17","dislikes":0,"thread":"6595120245","numReports":0,"likes":0,"message":"\u003cp>Hey Gus, did you end up getting this code to work? Did you want to compare results?\u003c/p>","id":"4986690707","createdAt":"2020-07-10T13:25:17","author":{"username":"disqus_vsvfcRoRBq","about":"","name":"Paul Nguyen","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2020-07-10T13:23:24","profileUrl":"https://disqus.com/by/disqus_vsvfcRoRBq/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"352815913","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_vsvfcRoRBq.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_vsvfcRoRBq.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_vsvfcRoRBq.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_vsvfcRoRBq.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4885888969,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Hey Gus, did you end up getting this code to work? Did you want to compare results?","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-08-08T02:09:31","dislikes":1,"thread":"6595120245","numReports":0,"likes":7,"message":"\u003cp>Question: Why does MultiHeadAttention class create only 4 clones instead of 8?\u003cbr>        self.linears = clones(nn.Linear(d_model, d_model), 4)\u003c/p>\u003cp>Thanks for a great post !\u003c/p>","id":"4015929276","createdAt":"2018-08-01T02:09:31","author":{"username":"kaustubhkunte","about":"","name":"Kaustubh Kunte","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-05-25T03:45:56","profileUrl":"https://disqus.com/by/kaustubhkunte/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"252947050","avatar":{"permalink":"https://disqus.com/api/users/avatars/kaustubhkunte.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/kaustubhkunte.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/kaustubhkunte.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/kaustubhkunte.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Question: Why does MultiHeadAttention class create only 4 clones instead of 8?\n        self.linears = clones(nn.Linear(d_model, d_model), 4)\n\nThanks for a great post !","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":0,"points":6,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-10-25T05:21:04","dislikes":0,"thread":"6595120245","numReports":0,"likes":11,"message":"\u003cp>I guess you misunderstand the zip function.\u003cbr>query, key, value = \\\u003cbr>            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\u003cbr>             for l, x in zip(self.linears, (query, key, value))]\u003cbr>In this code, the first linear layer works on query, the second one works on key and the third one works on value. The fourth one is used in: return self.linears[-1](x)\u003cbr>So the total number of linear layers is four.\u003c/p>","id":"4150577140","createdAt":"2018-10-18T05:21:04","author":{"username":"disqus_5iwwxhLeGL","about":"","name":"Yu-Tao","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2018-10-18T04:43:54","profileUrl":"https://disqus.com/by/disqus_5iwwxhLeGL/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"323780185","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_5iwwxhLeGL.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_5iwwxhLeGL.jpg","cache":"https://c.disquscdn.com/uploads/users/32378/185/avatar128.jpg?1539937069"},"cache":"https://c.disquscdn.com/uploads/users/32378/185/avatar92.jpg?1539937069","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_5iwwxhLeGL.jpg","cache":"https://c.disquscdn.com/uploads/users/32378/185/avatar92.jpg?1539937069"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_5iwwxhLeGL.jpg","cache":"https://c.disquscdn.com/uploads/users/32378/185/avatar32.jpg?1539937069"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4015929276,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"I guess you misunderstand the zip function.\nquery, key, value = \\\n            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n             for l, x in zip(self.linears, (query, key, value))]\nIn this code, the first linear layer works on query, the second one works on key and the third one works on value. The fourth one is used in: return self.linears[-1](x)\nSo the total number of linear layers is four.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":1,"points":11,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2019-02-27T23:20:56","dislikes":0,"thread":"6595120245","numReports":0,"likes":1,"message":"\u003cp>Hi \u003cbr>Why the linear layer has dimension (d_model, d_model) ? The  W^Q, W^K, and W^V have dimension (d_model, d_k), (d_model, d_k), and (d_model, d_v)? I do not see any layer corresponding to any of the three matrices.\u003c/p>","id":"4347070890","createdAt":"2019-02-20T23:20:56","author":{"username":"nimning","about":"","name":"nimning","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2019-02-20T23:20:25","profileUrl":"https://disqus.com/by/nimning/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"329113907","avatar":{"permalink":"https://disqus.com/api/users/avatars/nimning.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/nimning.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/nimning.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/nimning.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4150577140,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Hi \nWhy the linear layer has dimension (d_model, d_model) ? The  W^Q, W^K, and W^V have dimension (d_model, d_k), (d_model, d_k), and (d_model, d_v)? I do not see any layer corresponding to any of the three matrices.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":2,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2019-03-08T05:14:23","dislikes":0,"thread":"6595120245","numReports":0,"likes":4,"message":"\u003cp>This is my understanding,\u003cbr>see the code: \u003cbr>query, key, value = \\\u003c/p>\u003cp>            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))]\u003cbr>this will let the query, key,value pass through the first three Linear layers in self.linears, \u003cbr>so the first three Linear layers will be the Wq,Wk and Wv as you mentioned.\u003cbr>after this the matrix of query,key and value will have the shape(num_batches, time_steps, d_model).\u003c/p>\u003cp>Now since d_k = d_model//h, by reshaping it to(num_batches, -1, h, d_k) will give you the same amount of data.(here, d_k = d_v = d_w)\u003cbr> You can think of this as taking the first d_v features from d_model as the first head, then d_v+1 to 2*d_v as the second head. So now we do have 8 heads just by splitting the features of size d_model that we learnt from the matrix.\u003c/p>\u003cp>Then by doing this: transpose(1, 2), you will get tensor of shape (n_batches, h, time steps, d_k), and by feeding query, key and value to self_attention function, you will get back a tensor of shape(n_batches, h, time_steps, d_k).\u003c/p>\u003cp>you can see now that by doing transpose(1,2) and reshaping it to (n_batches, -1, d_k*h)\u003cbr>will get you a tensor with the same shape as previous layer.\u003c/p>\u003cp>So the main idea is that instead of creating 8*3 matrices for all values, keys and queries,\u003cbr>we only create 3 bigger ones and then split it to get the smaller ones.\u003c/p>","id":"4359258506","createdAt":"2019-03-01T05:14:23","author":{"username":"yipuding","about":"","name":"Yipu Ding","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2019-03-01T05:13:55","profileUrl":"https://disqus.com/by/yipuding/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"329461722","avatar":{"permalink":"https://disqus.com/api/users/avatars/yipuding.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/yipuding.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/yipuding.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/yipuding.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4347070890,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"This is my understanding,\nsee the code: \nquery, key, value = \\\n\n            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))]\nthis will let the query, key,value pass through the first three Linear layers in self.linears, \nso the first three Linear layers will be the Wq,Wk and Wv as you mentioned.\nafter this the matrix of query,key and value will have the shape(num_batches, time_steps, d_model). \n\nNow since d_k = d_model//h, by reshaping it to(num_batches, -1, h, d_k) will give you the same amount of data.(here, d_k = d_v = d_w)\n You can think of this as taking the first d_v features from d_model as the first head, then d_v+1 to 2*d_v as the second head. So now we do have 8 heads just by splitting the features of size d_model that we learnt from the matrix.\n\nThen by doing this: transpose(1, 2), you will get tensor of shape (n_batches, h, time steps, d_k), and by feeding query, key and value to self_attention function, you will get back a tensor of shape(n_batches, h, time_steps, d_k).\n\nyou can see now that by doing transpose(1,2) and reshaping it to (n_batches, -1, d_k*h)\nwill get you a tensor with the same shape as previous layer.\n\nSo the main idea is that instead of creating 8*3 matrices for all values, keys and queries,\nwe only create 3 bigger ones and then split it to get the smaller ones.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":3,"points":4,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2019-06-10T16:57:23","dislikes":0,"thread":"6595120245","numReports":0,"likes":0,"message":"\u003cp>Thanks for a great post! \u003cbr>It seems to me that instead of splitting the original matrix in 8 for the multi-head attention, we should be creating 8 independent learnt projections, so the code should be re-written accordingly - any thoughts?\u003c/p>","id":"4487791589","createdAt":"2019-06-03T16:57:23","author":{"username":"francoissteiner","about":"","name":"Francois Steiner","disable3rdPartyTrackers":true,"isPowerContributor":false,"joinedAt":"2019-06-03T16:56:22","profileUrl":"https://disqus.com/by/francoissteiner/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"333338486","avatar":{"permalink":"https://disqus.com/api/users/avatars/francoissteiner.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/francoissteiner.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/francoissteiner.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/francoissteiner.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4359258506,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Thanks for a great post! \nIt seems to me that instead of splitting the original matrix in 8 for the multi-head attention, we should be creating 8 independent learnt projections, so the code should be re-written accordingly - any thoughts?","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":4,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2019-10-02T15:39:44","dislikes":0,"thread":"6595120245","numReports":0,"likes":0,"message":"\u003cp>I also believed the input would be projected entirely in 8 new vectors when I read the paper. Question is: to create 8 independent learnt projections is really equivalent to splitting the original matrix in 8?\u003c/p>","id":"4628704871","createdAt":"2019-09-25T15:39:44","author":{"username":"disqus_KMuier91G9","about":"","name":"Sidney Melo","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2016-06-03T00:52:48","profileUrl":"https://disqus.com/by/disqus_KMuier91G9/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"210307358","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_KMuier91G9.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_KMuier91G9.jpg","cache":"https://c.disquscdn.com/uploads/users/21030/7358/avatar128.jpg?1569425986"},"cache":"https://c.disquscdn.com/uploads/users/21030/7358/avatar92.jpg?1569425986","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_KMuier91G9.jpg","cache":"https://c.disquscdn.com/uploads/users/21030/7358/avatar92.jpg?1569425986"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_KMuier91G9.jpg","cache":"https://c.disquscdn.com/uploads/users/21030/7358/avatar32.jpg?1569425986"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4487791589,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"I also believed the input would be projected entirely in 8 new vectors when I read the paper. Question is: to create 8 independent learnt projections is really equivalent to splitting the original matrix in 8?","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":5,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2019-10-16T02:49:48","dislikes":0,"thread":"6595120245","numReports":0,"likes":9,"message":"\u003cp>it is equivalent\uff0c an example below\uff1a\u003cbr> \u003ca href=\"https://uploads.disquscdn.com/images/958f686f492ff60d3d74ff4f735fec3a7910508849569a4edf0457471237ea54.jpg\" rel=\"nofollow noopener\" title=\"https://uploads.disquscdn.com/images/958f686f492ff60d3d74ff4f735fec3a7910508849569a4edf0457471237ea54.jpg\">https://uploads.disquscdn.c...\u003c/a>\u003c/p>","id":"4645489194","createdAt":"2019-10-09T02:49:48","author":{"username":"chuankangwu","about":"","name":"chuankang wu","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2019-10-06T07:25:11","profileUrl":"https://disqus.com/by/chuankangwu/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"338843326","avatar":{"permalink":"https://disqus.com/api/users/avatars/chuankangwu.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/chuankangwu.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/chuankangwu.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/chuankangwu.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[{"providerName":"Disquscdn","resolvedUrl":"https://uploads.disquscdn.com/images/958f686f492ff60d3d74ff4f735fec3a7910508849569a4edf0457471237ea54.jpg","thumbnailUrl":"//uploads.disquscdn.com/images/958f686f492ff60d3d74ff4f735fec3a7910508849569a4edf0457471237ea54.jpg","htmlHeight":null,"id":60198300,"thumbnailWidth":3000,"title":"","htmlWidth":null,"mediaType":"2","html":"","location":"https://uploads.disquscdn.com/images/958f686f492ff60d3d74ff4f735fec3a7910508849569a4edf0457471237ea54.jpg","type":"5","metadata":{"create_method":"preview","thumbnail":"//uploads.disquscdn.com/images/958f686f492ff60d3d74ff4f735fec3a7910508849569a4edf0457471237ea54.jpg"},"urlRedirect":"https://uploads.disquscdn.com/images/958f686f492ff60d3d74ff4f735fec3a7910508849569a4edf0457471237ea54.jpg","description":"","post":"4645489194","thumbnailURL":"//uploads.disquscdn.com/images/958f686f492ff60d3d74ff4f735fec3a7910508849569a4edf0457471237ea54.jpg","thread":"6595120245","forum":"harvard-nlp","url":"https://uploads.disquscdn.com/images/958f686f492ff60d3d74ff4f735fec3a7910508849569a4edf0457471237ea54.jpg","resolvedUrlRedirect":"https://uploads.disquscdn.com/images/958f686f492ff60d3d74ff4f735fec3a7910508849569a4edf0457471237ea54.jpg","thumbnailHeight":4000}],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4628704871,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"it is equivalent\uff0c an example below\uff1a\n https://uploads.disquscdn.com/images/958f686f492ff60d3d74ff4f735fec3a7910508849569a4edf0457471237ea54.jpg","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":6,"points":9,"moderationLabels":["media"],"isEdited":false,"sb":false},{"editableUntil":"2021-03-28T05:12:12","dislikes":0,"thread":"6595120245","numReports":0,"likes":0,"message":"\u003cp>Thanks for your answer\uff01\u003c/p>","id":"5311741227","createdAt":"2021-03-21T05:12:12","author":{"username":"disqus_vHGFqHGNX9","about":"","name":"\u6d77\u5c14","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2019-12-16T07:45:54","profileUrl":"https://disqus.com/by/disqus_vHGFqHGNX9/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"342134363","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_vHGFqHGNX9.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_vHGFqHGNX9.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_vHGFqHGNX9.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_vHGFqHGNX9.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4150577140,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Thanks for your answer\uff01","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":2,"points":0,"moderationLabels":[],"isEdited":true,"sb":false},{"editableUntil":"2019-07-28T07:51:46","dislikes":0,"thread":"6595120245","numReports":0,"likes":3,"message":"\u003cp>For the `make_std_mask` method:\u003cbr>`tgt_mask` have the shape of (1, batches, length), and the `subsequent_mask` method should output shape of (1, length, length). How the two tensors have `&amp;` operator?\u003c/p>","id":"4547747695","createdAt":"2019-07-21T07:51:46","author":{"username":"disqus_8lgrEHvH9y","about":"","name":"Frank Wang","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2019-07-21T07:43:53","profileUrl":"https://disqus.com/by/disqus_8lgrEHvH9y/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"335232778","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_8lgrEHvH9y.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_8lgrEHvH9y.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_8lgrEHvH9y.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_8lgrEHvH9y.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"For the `make_std_mask` method:\n`tgt_mask` have the shape of (1, batches, length), and the `subsequent_mask` method should output shape of (1, length, length). How the two tensors have `&` operator?","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":0,"points":3,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2020-03-27T14:14:18","dislikes":0,"thread":"6595120245","numReports":0,"likes":0,"message":"\u003cp>Have you found an answer?\u003c/p>","id":"4840517822","createdAt":"2020-03-20T14:14:18","author":{"username":"kenenbekarzymatov","about":"","name":"Kenenbek Arzymatov","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-03-17T01:45:41","profileUrl":"https://disqus.com/by/kenenbekarzymatov/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"149510443","avatar":{"permalink":"https://disqus.com/api/users/avatars/kenenbekarzymatov.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/kenenbekarzymatov.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/kenenbekarzymatov.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/kenenbekarzymatov.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4547747695,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Have you found an answer?","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2020-07-23T15:07:02","dislikes":0,"thread":"6595120245","numReports":0,"likes":1,"message":"\u003cp>I was stuck in the same question, but then I ran the the Batch class with a sample data, turns out the `tgt_mask` after unsqueeze has size: (nbatch,1, length) and the `subsequent_mask` method should output shape is (1, length, length), and it looks like the pytorch \"&amp;\" operation (unlike tensorflow) automatically tiles the first tensor along 0 dimension and the second tensor along 1st dimension and gives an output of size: (nbatch, length, length), this is intuitive since we want to have masking on every row in the batch.\u003c/p>","id":"4994173476","createdAt":"2020-07-16T15:07:02","author":{"username":"vishwaaschandan","about":"","name":"vishwaas chandan","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-09-30T10:10:37","profileUrl":"https://disqus.com/by/vishwaaschandan/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"176744344","avatar":{"permalink":"https://disqus.com/api/users/avatars/vishwaaschandan.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/vishwaaschandan.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/vishwaaschandan.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/vishwaaschandan.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4840517822,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"I was stuck in the same question, but then I ran the the Batch class with a sample data, turns out the `tgt_mask` after unsqueeze has size: (nbatch,1, length) and the `subsequent_mask` method should output shape is (1, length, length), and it looks like the pytorch \"&\" operation (unlike tensorflow) automatically tiles the first tensor along 0 dimension and the second tensor along 1st dimension and gives an output of size: (nbatch, length, length), this is intuitive since we want to have masking on every row in the batch.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":2,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2020-07-24T07:50:47","dislikes":0,"thread":"6595120245","numReports":0,"likes":1,"message":"\u003cp>Could you try out different inputs for the \"copy and paste\" task? \u003cbr>src = Variable(torch.LongTensor([[1,2,3,4,5,6,7,8,9,10]])) I have been getting different outputs different to the inputs. Which is really strange since my translation task works just fine..... here my version of the code for pytorch 1.5.1 \u003ca href=\"https://disq.us/url?url=https%3A%2F%2Fgithub.com%2Fmathematicsofpaul%2Ftransformer-update%3AixOcvBXLT-RexD2XT0QyvxC_lTk&amp;cuid=5460729\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://github.com/mathematicsofpaul/transformer-update\">https://github.com/mathemat...\u003c/a>\u003c/p>","id":"4995122656","createdAt":"2020-07-17T07:50:47","author":{"username":"disqus_vsvfcRoRBq","about":"","name":"Paul Nguyen","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2020-07-10T13:23:24","profileUrl":"https://disqus.com/by/disqus_vsvfcRoRBq/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"352815913","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_vsvfcRoRBq.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_vsvfcRoRBq.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_vsvfcRoRBq.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_vsvfcRoRBq.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4994173476,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Could you try out different inputs for the \"copy and paste\" task? \nsrc = Variable(torch.LongTensor([[1,2,3,4,5,6,7,8,9,10]])) I have been getting different outputs different to the inputs. Which is really strange since my translation task works just fine..... here my version of the code for pytorch 1.5.1 https://github.com/mathematicsofpaul/transformer-update","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":3,"points":1,"moderationLabels":["links"],"isEdited":false,"sb":false},{"editableUntil":"2018-08-02T10:24:08","dislikes":1,"thread":"6595120245","numReports":0,"likes":5,"message":"\u003cp>Thanks for great post!\u003cbr>Q: shouldn't it be mask.dim() &gt; 1 in LabelSmoothing#forward?\u003cbr>Because in your example with target [1] mask after nonzero([1] == 0) will be tensor([]) which has dimension 1 and it will throw error if we pass it as argument in index_fill_ as index.\u003c/p>","id":"4007049675","createdAt":"2018-07-26T10:24:08","author":{"username":"gaziev","about":"","name":"gaziev","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2012-03-24T01:49:52","profileUrl":"https://disqus.com/by/gaziev/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"23962751","avatar":{"permalink":"https://disqus.com/api/users/avatars/gaziev.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/gaziev.jpg","cache":"https://c.disquscdn.com/uploads/users/2396/2751/avatar128.jpg?1535563465"},"cache":"https://c.disquscdn.com/uploads/users/2396/2751/avatar92.jpg?1535563465","large":{"permalink":"https://disqus.com/api/users/avatars/gaziev.jpg","cache":"https://c.disquscdn.com/uploads/users/2396/2751/avatar92.jpg?1535563465"},"small":{"permalink":"https://disqus.com/api/users/avatars/gaziev.jpg","cache":"https://c.disquscdn.com/uploads/users/2396/2751/avatar32.jpg?1535563465"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Thanks for great post!\nQ: shouldn't it be mask.dim() > 1 in LabelSmoothing#forward?\nBecause in your example with target [1] mask after nonzero([1] == 0) will be tensor([]) which has dimension 1 and it will throw error if we pass it as argument in index_fill_ as index.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":0,"points":4,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-11-24T07:27:22","dislikes":1,"thread":"6595120245","numReports":0,"likes":6,"message":"\u003cp>mask.sum() &gt; 0 \uff1f\u003c/p>","id":"4199309870","createdAt":"2018-11-17T07:27:22","author":{"username":"lipiji1986","about":"","name":"Piji Li","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2012-10-08T12:42:12","profileUrl":"https://disqus.com/by/lipiji1986/","url":"http://www.zhizhihu.com/","location":"","isPrivate":false,"signedUrl":"http://disq.us/?url=http%3A%2F%2Fwww.zhizhihu.com%2F&key=7HVkbiv6ekcyrtTRqE3juQ","isPrimary":true,"isAnonymous":false,"id":"34038110","avatar":{"permalink":"https://disqus.com/api/users/avatars/lipiji1986.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/lipiji1986.jpg","cache":"https://c.disquscdn.com/uploads/users/3403/8110/avatar128.jpg?1542439643"},"cache":"https://c.disquscdn.com/uploads/users/3403/8110/avatar92.jpg?1542439643","large":{"permalink":"https://disqus.com/api/users/avatars/lipiji1986.jpg","cache":"https://c.disquscdn.com/uploads/users/3403/8110/avatar92.jpg?1542439643"},"small":{"permalink":"https://disqus.com/api/users/avatars/lipiji1986.jpg","cache":"https://c.disquscdn.com/uploads/users/3403/8110/avatar32.jpg?1542439643"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4007049675,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"mask.sum() > 0 \uff1f","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":1,"points":5,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-11-27T03:02:18","dislikes":1,"thread":"6595120245","numReports":0,"likes":0,"message":"\u003cp>change it to \"mask.sum() &gt; 0 and len(mask) &gt; 0\", it works!\u003c/p>","id":"4203482166","createdAt":"2018-11-20T03:02:18","author":{"username":"disqus_W48rgB1BxP","about":"","name":"Ke Wang","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2018-11-20T03:01:41","profileUrl":"https://disqus.com/by/disqus_W48rgB1BxP/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"325338843","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_W48rgB1BxP.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_W48rgB1BxP.jpg","cache":"https://c.disquscdn.com/uploads/users/32533/8843/avatar128.jpg?1542682939"},"cache":"https://c.disquscdn.com/uploads/users/32533/8843/avatar92.jpg?1542682939","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_W48rgB1BxP.jpg","cache":"https://c.disquscdn.com/uploads/users/32533/8843/avatar92.jpg?1542682939"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_W48rgB1BxP.jpg","cache":"https://c.disquscdn.com/uploads/users/32533/8843/avatar32.jpg?1542682939"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4199309870,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"change it to \"mask.sum() > 0 and len(mask) > 0\", it works!","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":2,"points":-1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-10-26T08:17:48","dislikes":0,"thread":"6595120245","numReports":0,"likes":0,"message":"\u003cp>Same problem. I solve it by replace MyIterator with data.BucketIterator.\u003cbr>I'm not sure if it is related with the batch data.\u003c/p>","id":"4152328935","createdAt":"2018-10-19T08:17:48","author":{"username":"disqus_5iwwxhLeGL","about":"","name":"Yu-Tao","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2018-10-18T04:43:54","profileUrl":"https://disqus.com/by/disqus_5iwwxhLeGL/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"323780185","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_5iwwxhLeGL.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_5iwwxhLeGL.jpg","cache":"https://c.disquscdn.com/uploads/users/32378/185/avatar128.jpg?1539937069"},"cache":"https://c.disquscdn.com/uploads/users/32378/185/avatar92.jpg?1539937069","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_5iwwxhLeGL.jpg","cache":"https://c.disquscdn.com/uploads/users/32378/185/avatar92.jpg?1539937069"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_5iwwxhLeGL.jpg","cache":"https://c.disquscdn.com/uploads/users/32378/185/avatar32.jpg?1539937069"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4007049675,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Same problem. I solve it by replace MyIterator with data.BucketIterator.\nI'm not sure if it is related with the batch data.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2019-03-01T08:16:26","dislikes":1,"thread":"6595120245","numReports":0,"likes":4,"message":"\u003cp>The output of each step in the decoder is fed to the bottom decoder in the next time step. So, based on thsi implementation, how can the decoder generate word one by one based on previous prediction? I do not see this logic in the code.\u003c/p>","id":"4349056928","createdAt":"2019-02-22T08:16:26","author":{"username":"disqus_pcWVloYWqZ","about":"","name":"Ning Ma","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-11-13T08:57:59","profileUrl":"https://disqus.com/by/disqus_pcWVloYWqZ/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"182623577","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_pcWVloYWqZ.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_pcWVloYWqZ.jpg","cache":"https://c.disquscdn.com/uploads/users/18262/3577/avatar128.jpg?1550823388"},"cache":"https://c.disquscdn.com/uploads/users/18262/3577/avatar92.jpg?1550823388","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_pcWVloYWqZ.jpg","cache":"https://c.disquscdn.com/uploads/users/18262/3577/avatar92.jpg?1550823388"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_pcWVloYWqZ.jpg","cache":"https://c.disquscdn.com/uploads/users/18262/3577/avatar32.jpg?1550823388"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"The output of each step in the decoder is fed to the bottom decoder in the next time step. So, based on thsi implementation, how can the decoder generate word one by one based on previous prediction? I do not see this logic in the code.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":0,"points":3,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2019-06-27T21:29:34","dislikes":0,"thread":"6595120245","numReports":0,"likes":1,"message":"\u003cp>During training, we don't need to feed the predicted word to the bottom decoder. We use teacher forcing here and feed the true word in the next time step. During testing though, we have to feed the predicted word (till &lt;eos&gt;) to generate the sentence. Please take a look towards the end here: \u003ca href=\"https://disq.us/url?url=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-code-the-transformer-in-pytorch-24db27c8f9ec%3AbvDLL3adJKBQmU5c0DdyhSCRu2k&amp;cuid=5460729\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec\">https://towardsdatascience....\u003c/a>\u003c/p>","id":"4510068543","createdAt":"2019-06-20T21:29:34","author":{"username":"disqus_yCoVzKJPyj","about":"","name":"Ishaan","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2019-06-20T21:25:58","profileUrl":"https://disqus.com/by/disqus_yCoVzKJPyj/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"334049366","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_yCoVzKJPyj.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_yCoVzKJPyj.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_yCoVzKJPyj.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_yCoVzKJPyj.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4349056928,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"During training, we don't need to feed the predicted word to the bottom decoder. We use teacher forcing here and feed the true word in the next time step. During testing though, we have to feed the predicted word (till \u003ceos>) to generate the sentence. Please take a look towards the end here: https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":1,"points":1,"moderationLabels":["links"],"isEdited":true,"sb":false},{"editableUntil":"2019-04-16T18:51:59","dislikes":0,"thread":"6595120245","numReports":0,"likes":2,"message":"\u003cp>In PositionalEncoding, there is a small fix discussed here: \u003ca href=\"https://disq.us/url?url=https%3A%2F%2Fstackoverflow.com%2Fquestions%2F52922445%2Fruntimeerror-exp-not-implemented-for-torch-longtensor%3ABrNPJZE-73zYv0I0pJ8j2ZPvDEM&amp;cuid=5460729\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://stackoverflow.com/questions/52922445/runtimeerror-exp-not-implemented-for-torch-longtensor\">https://stackoverflow.com/q...\u003c/a>\u003c/p>\u003cp>LU Jialin quote:\u003c/p>\u003cp>For me I just got the torch.arange to generate float type tensor\u003c/p>\u003cp>from\u003c/p>\u003cp>position = torch.arange(0, max_len).unsqueeze(1)\u003cbr>div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\u003cbr>to\u003c/p>\u003cp>position = torch.arange(\u003cb>0.\u003c/b>, max_len).unsqueeze(1)\u003cbr>div_term = torch.exp(torch.arange(\u003cb>0.\u003c/b>, d_model, 2) * -(math.log(10000.0) / d_model))\u003c/p>","id":"4416166916","createdAt":"2019-04-09T18:51:59","author":{"username":"disqus_T9X568BHlO","about":"","name":"Arthur Marques","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-08-01T03:15:21","profileUrl":"https://disqus.com/by/disqus_T9X568BHlO/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"260788705","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_T9X568BHlO.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_T9X568BHlO.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_T9X568BHlO.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_T9X568BHlO.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"In PositionalEncoding, there is a small fix discussed here: https://stackoverflow.com/questions/52922445/runtimeerror-exp-not-implemented-for-torch-longtensor\n\nLU Jialin quote:\n\nFor me I just got the torch.arange to generate float type tensor\n\nfrom\n\nposition = torch.arange(0, max_len).unsqueeze(1)\ndiv_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\nto\n\nposition = torch.arange(\u003cb>0.\u003c/b>, max_len).unsqueeze(1)\ndiv_term = torch.exp(torch.arange(\u003cb>0.\u003c/b>, d_model, 2) * -(math.log(10000.0) / d_model))","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":0,"points":2,"moderationLabels":["links"],"isEdited":false,"sb":false},{"editableUntil":"2019-01-22T08:36:37","dislikes":0,"thread":"6595120245","numReports":0,"likes":2,"message":"\u003cp>Thanks a lot for such great post!\u003c/p>\u003cp>I guess there is a mistake in following part of code:\u003cbr>\u003ccode>\u003cbr>def clones(module, N):\u003cbr>         \"Produce N identical layers.\"\u003cbr>         return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\u003cbr>\u003c/code>\u003c/p>\u003cp>\"Deepcopy\" copies the entire layer with it's weights what is wrong because (Wq_1, Wq_2, Wq_3, ..., Wk_1, Wk_2, Wk_3, ..., etc) must be randomly initialized. In other words \"deepcopy\" copies the only instance of class nn.Linear but N instances must be created. So I suppose that it's better to replace it by something like that:\u003cbr>\u003ccode>\u003cbr>def clones(params, N):\u003cbr>     return nn.ModuleList([nn.Linear(params) for _ in range(N)])\u003cbr>\u003c/code>\u003c/p>\u003cp>Sincerely, \u003cbr>Ivan\u003c/p>","id":"4286765854","createdAt":"2019-01-15T08:36:37","author":{"username":"disqus_Wv15C2dVTm","about":"","name":"\u0418\u0432\u0430\u043d","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2014-02-10T21:55:14","profileUrl":"https://disqus.com/by/disqus_Wv15C2dVTm/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"94329578","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_Wv15C2dVTm.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_Wv15C2dVTm.jpg","cache":"https://c.disquscdn.com/uploads/users/9432/9578/avatar128.jpg?1392121832"},"cache":"https://c.disquscdn.com/uploads/users/9432/9578/avatar92.jpg?1392121832","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_Wv15C2dVTm.jpg","cache":"https://c.disquscdn.com/uploads/users/9432/9578/avatar92.jpg?1392121832"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_Wv15C2dVTm.jpg","cache":"https://c.disquscdn.com/uploads/users/9432/9578/avatar32.jpg?1392121832"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Thanks a lot for such great post!\n\nI guess there is a mistake in following part of code:\n\u003ccode>\ndef clones(module, N):\n         \"Produce N identical layers.\"\n         return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n\u003c/code>\n\n\"Deepcopy\" copies the entire layer with it's weights what is wrong because (Wq_1, Wq_2, Wq_3, ..., Wk_1, Wk_2, Wk_3, ..., etc) must be randomly initialized. In other words \"deepcopy\" copies the only instance of class nn.Linear but N instances must be created. So I suppose that it's better to replace it by something like that:\n\u003ccode>\ndef clones(params, N):\n     return nn.ModuleList([nn.Linear(params) for _ in range(N)])\n\u003c/code>\n\nSincerely, \nIvan","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":0,"points":2,"moderationLabels":[],"isEdited":true,"sb":false},{"editableUntil":"2019-03-01T12:33:48","dislikes":0,"thread":"6595120245","numReports":0,"likes":0,"message":"\u003cp>Later there is code for model initialization at line 20 in the notebook\u003c/p>","id":"4349244276","createdAt":"2019-02-22T12:33:48","author":{"username":"peixiangzhong","about":"","name":"Peixiang Zhong","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2016-04-03T03:35:07","profileUrl":"https://disqus.com/by/peixiangzhong/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"202886301","avatar":{"permalink":"https://disqus.com/api/users/avatars/peixiangzhong.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/peixiangzhong.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/peixiangzhong.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/peixiangzhong.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4286765854,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Later there is code for model initialization at line 20 in the notebook","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2022-03-20T09:32:11","dislikes":0,"thread":"6595120245","numReports":0,"likes":1,"message":"\u003cp>The implementation of Embedding seems incorrect. \u003cbr>\u003ccode>return self.lut(x) * math.sqrt(self.d_model)\u003c/code>\u003cbr>The original paper mentioned multiplying by \"sqrt(d_model)\" because in Tensorflow implementation, the embedding weights are initialized to the inverse of sqrt(d_model), so they scale it back to N(0, 1). But in pytorch the initial embedding weights are already N(0,1), so multiplying by sqrt(d_model) will make the weights unreasonably large.\u003c/p>","id":"5787940603","createdAt":"2022-03-13T09:32:11","author":{"username":"runqiyang","about":"","name":"Runqi Yang","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-05-21T03:26:02","profileUrl":"https://disqus.com/by/runqiyang/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"252521167","avatar":{"permalink":"https://disqus.com/api/users/avatars/runqiyang.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/runqiyang.jpg","cache":"https://c.disquscdn.com/uploads/users/25252/1167/avatar200.jpg?1647163932"},"cache":"https://c.disquscdn.com/uploads/users/25252/1167/avatar92.jpg?1647163932","large":{"permalink":"https://disqus.com/api/users/avatars/runqiyang.jpg","cache":"https://c.disquscdn.com/uploads/users/25252/1167/avatar92.jpg?1647163932"},"small":{"permalink":"https://disqus.com/api/users/avatars/runqiyang.jpg","cache":"https://c.disquscdn.com/uploads/users/25252/1167/avatar32.jpg?1647163932"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"The implementation of Embedding seems incorrect. \n\u003ccode>return self.lut(x) * math.sqrt(self.d_model)\u003c/code>\nThe original paper mentioned multiplying by \"sqrt(d_model)\" because in Tensorflow implementation, the embedding weights are initialized to the inverse of sqrt(d_model), so they scale it back to N(0, 1). But in pytorch the initial embedding weights are already N(0,1), so multiplying by sqrt(d_model) will make the weights unreasonably large.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":0,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2022-02-15T03:16:22","dislikes":0,"thread":"6595120245","numReports":0,"likes":1,"message":"\u003cp>Thanks for this great post.\u003c/p>\u003cp>I still have some doubts about the Mask, I know it is to prevent the subsequent positions from being seen during model training. But why does the Mask mechanism only appear in Decoder\u2019s self Attention? Shouldn\u2019t the same mask mechanism be introduced in other modules of Decoder?\u003c/p>\u003cp>Before the first Multi-Headed Attention module in Decoder, there is also a residual structure, which will bring the original Output Embedding matrix(with entire output groung truth tokens) into the subsequent \u201cADD &amp; Norm\u201d module, add this Masked matrix, and then perform following processing.\u003c/p>\u003cp>I think this will cause all GT Output tokens to be brought in in the subsequent processing, and will be passed all the way to the final output of the Decoder. Will this not lead to the leakage of the Ground Truth in the Linear and Softmax output stages?\u003c/p>","id":"5725761556","createdAt":"2022-02-08T03:16:22","author":{"username":"jimeverest","about":"","name":"Jim Everest","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2022-02-08T03:16:04","profileUrl":"https://disqus.com/by/jimeverest/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"381392369","avatar":{"permalink":"https://disqus.com/api/users/avatars/jimeverest.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/jimeverest.jpg","cache":"https://c.disquscdn.com/uploads/users/38139/2369/avatar200.jpg?1696925783"},"cache":"https://c.disquscdn.com/uploads/users/38139/2369/avatar92.jpg?1696925783","large":{"permalink":"https://disqus.com/api/users/avatars/jimeverest.jpg","cache":"https://c.disquscdn.com/uploads/users/38139/2369/avatar92.jpg?1696925783"},"small":{"permalink":"https://disqus.com/api/users/avatars/jimeverest.jpg","cache":"https://c.disquscdn.com/uploads/users/38139/2369/avatar32.jpg?1696925783"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Thanks for this great post.\n\nI still have some doubts about the Mask, I know it is to prevent the subsequent positions from being seen during model training. But why does the Mask mechanism only appear in Decoder\u2019s self Attention? Shouldn\u2019t the same mask mechanism be introduced in other modules of Decoder?\n\nBefore the first Multi-Headed Attention module in Decoder, there is also a residual structure, which will bring the original Output Embedding matrix(with entire output groung truth tokens) into the subsequent \u201cADD & Norm\u201d module, add this Masked matrix, and then perform following processing.\n\nI think this will cause all GT Output tokens to be brought in in the subsequent processing, and will be passed all the way to the final output of the Decoder. Will this not lead to the leakage of the Ground Truth in the Linear and Softmax output stages?","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":0,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2021-04-25T18:09:53","dislikes":0,"thread":"6595120245","numReports":0,"likes":1,"message":"\u003cp>When doing the datasets.IWSLT.splits,  I have the following error : OSError: Not a gzipped file (b'\u003c/p>","id":"5349873650","createdAt":"2021-04-18T18:09:53","author":{"username":"gilles_jack","about":"","name":"Gilles","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2014-04-07T07:13:17","profileUrl":"https://disqus.com/by/gilles_jack/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"102092859","avatar":{"permalink":"https://disqus.com/api/users/avatars/gilles_jack.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/gilles_jack.jpg","cache":"https://c.disquscdn.com/uploads/users/10209/2859/avatar128.jpg?1528911528"},"cache":"https://c.disquscdn.com/uploads/users/10209/2859/avatar92.jpg?1528911528","large":{"permalink":"https://disqus.com/api/users/avatars/gilles_jack.jpg","cache":"https://c.disquscdn.com/uploads/users/10209/2859/avatar92.jpg?1528911528"},"small":{"permalink":"https://disqus.com/api/users/avatars/gilles_jack.jpg","cache":"https://c.disquscdn.com/uploads/users/10209/2859/avatar32.jpg?1528911528"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"When doing the datasets.IWSLT.splits,  I have the following error : OSError: Not a gzipped file (b'\u003c!'). Do you have a fix please? PS: I replaced torchtext for torchtext.legacy","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":0,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2021-07-16T21:10:23","dislikes":0,"thread":"6595120245","numReports":0,"likes":0,"message":"\u003cp>Same issue for me. It seems that the functionality of torchtext 0.10.0 has changed without backward compatibility and the code in this post has to be adapted to load the IWSLT database. Has someone figured out how to solve this? Please share. Thanks!\u003c/p>","id":"5450034176","createdAt":"2021-07-09T21:10:23","author":{"username":"christophwindheuser","about":"","name":"Christoph Windheuser","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2016-12-26T22:05:37","profileUrl":"https://disqus.com/by/christophwindheuser/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"236438827","avatar":{"permalink":"https://disqus.com/api/users/avatars/christophwindheuser.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/christophwindheuser.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/christophwindheuser.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/christophwindheuser.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":5349873650,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Same issue for me. It seems that the functionality of torchtext 0.10.0 has changed without backward compatibility and the code in this post has to be adapted to load the IWSLT database. Has someone figured out how to solve this? Please share. Thanks!","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2021-05-08T08:38:17","dislikes":0,"thread":"6595120245","numReports":0,"likes":0,"message":"\u003cp>Same issue, have you resolved it please? Again I was trying to run with local datasets but I can't split with torch as specified as in the code. \u003cbr> (train, val, test = datasets.IWSLT.splits(\u003cbr>    exts=('.en', '.de'), fields=(SRC, TGT), \u003cbr>    filter_pred=lambda x: len(vars(x)['src']) &lt;= MAX_LEN \u003cbr>    and len(vars(x)['trg']) &lt;= MAX_LEN))\u003cbr>How can I use this fragment for a local dataset please help.\u003c/p>","id":"5366533776","createdAt":"2021-05-01T08:38:17","author":{"username":"amdeworkasefa","about":"","name":"Amdework Asefa Zemaryam","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2021-05-01T08:20:31","profileUrl":"https://disqus.com/by/amdeworkasefa/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"368669535","avatar":{"permalink":"https://disqus.com/api/users/avatars/amdeworkasefa.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/amdeworkasefa.jpg","cache":"https://c.disquscdn.com/uploads/users/36866/9535/avatar128.jpg?1619858813"},"cache":"https://c.disquscdn.com/uploads/users/36866/9535/avatar92.jpg?1619858813","large":{"permalink":"https://disqus.com/api/users/avatars/amdeworkasefa.jpg","cache":"https://c.disquscdn.com/uploads/users/36866/9535/avatar92.jpg?1619858813"},"small":{"permalink":"https://disqus.com/api/users/avatars/amdeworkasefa.jpg","cache":"https://c.disquscdn.com/uploads/users/36866/9535/avatar32.jpg?1619858813"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":5349873650,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Same issue, have you resolved it please? Again I was trying to run with local datasets but I can't split with torch as specified as in the code. \n (train, val, test = datasets.IWSLT.splits(\n    exts=('.en', '.de'), fields=(SRC, TGT), \n    filter_pred=lambda x: len(vars(x)['src']) \u003c= MAX_LEN \n    and len(vars(x)['trg']) \u003c= MAX_LEN))\nHow can I use this fragment for a local dataset please help.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2021-04-26T03:14:37","dislikes":0,"thread":"6595120245","numReports":0,"likes":0,"message":"\u003cp>I have the same question as yours, do you figure it out\u003c/p>","id":"5350335350","createdAt":"2021-04-19T03:14:37","author":{"username":"ybli","about":"","name":"yb li","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2021-04-19T03:14:28","profileUrl":"https://disqus.com/by/ybli/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"367985524","avatar":{"permalink":"https://disqus.com/api/users/avatars/ybli.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/ybli.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/ybli.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/ybli.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":5349873650,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"I have the same question as yours, do you figure it out","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2021-05-08T08:46:53","dislikes":0,"thread":"6595120245","numReports":0,"likes":0,"message":"\u003cp>not yet, as soon as you got it. please share us! thank you!\u003c/p>","id":"5366537777","createdAt":"2021-05-01T08:46:53","author":{"username":"amdeworkasefa","about":"","name":"Amdework Asefa Zemaryam","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2021-05-01T08:20:31","profileUrl":"https://disqus.com/by/amdeworkasefa/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"368669535","avatar":{"permalink":"https://disqus.com/api/users/avatars/amdeworkasefa.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/amdeworkasefa.jpg","cache":"https://c.disquscdn.com/uploads/users/36866/9535/avatar128.jpg?1619858813"},"cache":"https://c.disquscdn.com/uploads/users/36866/9535/avatar92.jpg?1619858813","large":{"permalink":"https://disqus.com/api/users/avatars/amdeworkasefa.jpg","cache":"https://c.disquscdn.com/uploads/users/36866/9535/avatar92.jpg?1619858813"},"small":{"permalink":"https://disqus.com/api/users/avatars/amdeworkasefa.jpg","cache":"https://c.disquscdn.com/uploads/users/36866/9535/avatar32.jpg?1619858813"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":5350335350,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"not yet, as soon as you got it. please share us! thank you!","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":2,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2021-07-17T09:10:58","dislikes":0,"thread":"6595120245","numReports":0,"likes":1,"message":"\u003cp>I solved this problem, just go to this website: \u003ca href=\"https://disq.us/url?url=https%3A%2F%2Fwit3.fbk.eu%2F2016-01%3ACBSIOv8E4K-F33qrmUDVnUngo-o&amp;cuid=5460729\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://wit3.fbk.eu/2016-01\">https://wit3.fbk.eu/2016-01\u003c/a>, click \"link\" and download the \"2016-01.tgz\" file. In this file you can find the \"2016-01/texts/de/en/de-en.tgz\" file. Simply put this file in \"your_work_dir/.data/iwslt/\", then it works fine.\u003c/p>","id":"5450509664","createdAt":"2021-07-10T09:10:58","author":{"username":"disqus_RN5nYClnUa","about":"","name":"Benjamin","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2021-07-10T09:10:16","profileUrl":"https://disqus.com/by/disqus_RN5nYClnUa/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"372134232","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_RN5nYClnUa.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_RN5nYClnUa.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_RN5nYClnUa.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_RN5nYClnUa.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":5366537777,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"I solved this problem, just go to this website: https://wit3.fbk.eu/2016-01, click \"link\" and download the \"2016-01.tgz\" file. In this file you can find the \"2016-01/texts/de/en/de-en.tgz\" file. Simply put this file in \"your_work_dir/.data/iwslt/\", then it works fine.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":3,"points":1,"moderationLabels":["links"],"isEdited":true,"sb":false},{"editableUntil":"2021-11-24T15:03:01","dislikes":0,"thread":"6595120245","numReports":0,"likes":1,"message":"\u003cp>\u003cb>For the struggling ones in the future.\u003c/b>\u003cbr>When you run the code below, it downloads the data-set in the directory mentioned in \u003cb>\"root\" address. \u003c/b>\u003cbr>Name any directory and it will create a folder with that name, then it proceeds to download the content \"de-en.tgz\" in \u003cb>/myfolder/iwslt/\u003c/b>\u003cbr>\u003ccode>datasets.IWSLT.splits(exts=('.de', '.en'),\u003cbr>fields=(SRC, TGT),\u003cbr>\u003cb>root=\"/myfolder/\",\u003c/b>\u003cbr>filter_pred= lambda x: len(vars(x)['src']) &lt;= MAX_LEN and  len(vars(x)['trg']) &lt;= MAX_LEN)\u003c/code>\u003cbr>Now, delete this old \"de-en.tgz\" file in /myfolder/iwslt/, and download tgz file mentioned above from the link, now after extracting, go to 2016-01/texts/de/en/\u003cbr>\u003cb>Copypaste the new tgz file into /myfolder/iwslt/.\u003c/b>\u003cbr>Now, run the code again.\u003c/p>","id":"5611852775","createdAt":"2021-11-17T15:03:01","author":{"username":"stangerdanger","about":"","name":"Stanger Danger","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2021-11-17T14:48:46","profileUrl":"https://disqus.com/by/stangerdanger/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"378404535","avatar":{"permalink":"https://disqus.com/api/users/avatars/stangerdanger.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/stangerdanger.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/stangerdanger.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/stangerdanger.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":5450509664,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"\u003cb>For the struggling ones in the future.\u003c/b>\nWhen you run the code below, it downloads the data-set in the directory mentioned in \u003cb>\"root\" address. \u003c/b>\nName any directory and it will create a folder with that name, then it proceeds to download the content \"de-en.tgz\" in \u003cb>/myfolder/iwslt/\u003c/b>\n\u003ccode>datasets.IWSLT.splits(exts=('.de', '.en'),\nfields=(SRC, TGT),\n\u003cb>root=\"/myfolder/\",\u003c/b>\nfilter_pred= lambda x: len(vars(x)['src']) \u003c= MAX_LEN and  len(vars(x)['trg']) \u003c= MAX_LEN)\u003c/code>\nNow, delete this old \"de-en.tgz\" file in /myfolder/iwslt/, and download tgz file mentioned above from the link, now after extracting, go to 2016-01/texts/de/en/\n\u003cb>Copypaste the new tgz file into /myfolder/iwslt/.\u003c/b>\nNow, run the code again.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":4,"points":1,"moderationLabels":[],"isEdited":true,"sb":false},{"editableUntil":"2020-04-27T01:16:43","dislikes":0,"thread":"6595120245","numReports":0,"likes":1,"message":"\u003cp>Thanks for the great post\u003c/p>\u003cp>I\u2019m using standard TRANSFORMER for NMT and I\u2019m going to train model Right to left. I have applied two ideas:\u003c/p>\u003cp>1. reversing the input text from left to Right, before feeding the data to \u003cbr> the encoder and decoder. (but the training process is still left to right)\u003c/p>\u003cp>2. reversing the embedding vector in the encoder and decoder. (Result is so boor, and the BLUE score is decreased by 15.6 points)\u003c/p>\u003cp>My question is what the optimal way to train model Right to Left?\u003c/p>\u003cp>Note: I have two versions of my model (RTL model) and (LTR model).\u003c/p>","id":"4881933302","createdAt":"2020-04-20T01:16:43","author":{"username":"aimanmutasembellh","about":"","name":"Aiman Mutasem-bellh","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2014-12-23T16:12:54","profileUrl":"https://disqus.com/by/aimanmutasembellh/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"136724852","avatar":{"permalink":"https://disqus.com/api/users/avatars/aimanmutasembellh.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/aimanmutasembellh.jpg","cache":"https://c.disquscdn.com/uploads/users/13672/4852/avatar128.jpg?1589804941"},"cache":"https://c.disquscdn.com/uploads/users/13672/4852/avatar92.jpg?1589804941","large":{"permalink":"https://disqus.com/api/users/avatars/aimanmutasembellh.jpg","cache":"https://c.disquscdn.com/uploads/users/13672/4852/avatar92.jpg?1589804941"},"small":{"permalink":"https://disqus.com/api/users/avatars/aimanmutasembellh.jpg","cache":"https://c.disquscdn.com/uploads/users/13672/4852/avatar32.jpg?1589804941"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Thanks for the great post \n\nI\u2019m using standard TRANSFORMER for NMT and I\u2019m going to train model Right to left. I have applied two ideas:\n\n1. reversing the input text from left to Right, before feeding the data to \n the encoder and decoder. (but the training process is still left to right)\n\n2. reversing the embedding vector in the encoder and decoder. (Result is so boor, and the BLUE score is decreased by 15.6 points)\n\nMy question is what the optimal way to train model Right to Left?\n\nNote: I have two versions of my model (RTL model) and (LTR model).","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":0,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2020-04-15T10:46:35","dislikes":0,"thread":"6595120245","numReports":0,"likes":1,"message":"\u003cp>Thank you for sharing so unselfishly. I have a question to ask you and I hope you can help me. About this tuple in Multihead attention:\u003c/p>\u003cp>\u003ccode>query, key, value = /\u003cbr> [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)  \u003cbr>for l, x in zip(self.linears, (query, key, value))]\u003c/code>\u003c/p>\u003cp>I've never seen a method that calls the element l(x) in a tuple like this, and I don't understand that very well.\u003c/p>\u003cp>For these three variables, \u201c\u201dquery, key, value\u201c\u201d\uff0c I didn't see the input values before in the forward of Class multihead attention, but I found that they had values during the run. Where did they get their values from?\u003c/p>\u003cp>In my understanding, I think the values of these three variables \u201c query, key, value\u201d are corresponding to the values of the first three self.linears, but it doesn't seem that way. I hope someone can give me some help to explain the details here. Thank you so much.\u003c/p>","id":"4865841600","createdAt":"2020-04-08T10:46:35","author":{"username":"disqus_W64TGqfzwI","about":"","name":"Salted Fish","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2020-04-08T04:10:23","profileUrl":"https://disqus.com/by/disqus_W64TGqfzwI/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"347313044","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_W64TGqfzwI.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_W64TGqfzwI.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_W64TGqfzwI.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_W64TGqfzwI.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Thank you for sharing so unselfishly. I have a question to ask you and I hope you can help me. About this tuple in Multihead attention:\n\n\u003ccode>query, key, value = /\n [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)  \nfor l, x in zip(self.linears, (query, key, value))]\u003c/code>\n\nI've never seen a method that calls the element l(x) in a tuple like this, and I don't understand that very well.\n\nFor these three variables, \u201c\u201dquery, key, value\u201c\u201d\uff0c I didn't see the input values before in the forward of Class multihead attention, but I found that they had values during the run. Where did they get their values from? \n\nIn my understanding, I think the values of these three variables \u201c query, key, value\u201d are corresponding to the values of the first three self.linears, but it doesn't seem that way. I hope someone can give me some help to explain the details here. Thank you so much.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":0,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2021-08-03T09:03:37","dislikes":0,"thread":"6595120245","numReports":0,"likes":0,"message":"\u003cp>Your understanding is right. Q,K,V are actually corresponding to the values of the first three self.linears.\u003c/p>","id":"5471043035","createdAt":"2021-07-27T09:03:37","author":{"username":"disqus_52IybZTuQO","about":"","name":"ice ice","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2021-07-27T09:01:10","profileUrl":"https://disqus.com/by/disqus_52IybZTuQO/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"372903320","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_52IybZTuQO.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_52IybZTuQO.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_52IybZTuQO.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_52IybZTuQO.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4865841600,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Your understanding is right. Q,K,V are actually corresponding to the values of the first three self.linears.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2019-11-17T01:26:33","dislikes":0,"thread":"6595120245","numReports":0,"likes":1,"message":"\u003cp>Thanks for the awesome post!\u003cbr>I have a question: there seems to be a small issue in the SimpleLossCompute function. loss.backward() accumulates gradient during evaluation but, they won't be set to zero before the next step. Won't the next training step use gradients computed during evaluation ?\u003c/p>","id":"4683799335","createdAt":"2019-11-10T01:26:33","author":{"username":"disqus_jVxRqA9Gkm","about":"","name":"Anshul Shah","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2019-11-10T01:25:43","profileUrl":"https://disqus.com/by/disqus_jVxRqA9Gkm/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"340520918","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_jVxRqA9Gkm.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_jVxRqA9Gkm.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_jVxRqA9Gkm.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_jVxRqA9Gkm.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Thanks for the awesome post!\nI have a question: there seems to be a small issue in the SimpleLossCompute function. loss.backward() accumulates gradient during evaluation but, they won't be set to zero before the next step. Won't the next training step use gradients computed during evaluation ?","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":0,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2020-04-30T04:13:53","dislikes":0,"thread":"6595120245","numReports":0,"likes":0,"message":"\u003cp>Yes, it happens as you stated. Running validation does increase the performance on the validation set compare to if one does not run the validation. See my post here on the same topic.\u003c/p>","id":"4886246116","createdAt":"2020-04-23T04:13:53","author":{"username":"disqus_wGGAF5yQKM","about":"","name":"Gus Smith","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2015-10-05T17:03:42","profileUrl":"https://disqus.com/by/disqus_wGGAF5yQKM/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"177403162","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_wGGAF5yQKM.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_wGGAF5yQKM.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_wGGAF5yQKM.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_wGGAF5yQKM.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4683799335,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Yes, it happens as you stated. Running validation does increase the performance on the validation set compare to if one does not run the validation. See my post here on the same topic.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":1,"points":0,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-12-12T21:17:03","dislikes":0,"thread":"6595120245","numReports":0,"likes":1,"message":"\u003cp>Hi, so I'm trying to train the model on the IWSLT en-de dataset on a 2 GPU machine each with 12G memory. But I am running out of GPU memory. Is this normal?\u003c/p>","id":"4227028789","createdAt":"2018-12-05T21:17:03","author":{"username":"farzadsharif","about":"","name":"Farzad Sharif","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2016-09-23T15:27:42","profileUrl":"https://disqus.com/by/farzadsharif/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"224156259","avatar":{"permalink":"https://disqus.com/api/users/avatars/farzadsharif.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/farzadsharif.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/farzadsharif.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/farzadsharif.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Hi, so I'm trying to train the model on the IWSLT en-de dataset on a 2 GPU machine each with 12G memory. But I am running out of GPU memory. Is this normal?","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":0,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2020-05-25T12:28:59","dislikes":0,"thread":"6595120245","numReports":0,"likes":1,"message":"\u003cp>Hello Mr. Farzad. Kindlly, did you fix this issue? :)\u003c/p>","id":"4918157349","createdAt":"2020-05-18T12:28:59","author":{"username":"aimanmutasembellh","about":"","name":"Aiman Mutasem-bellh","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2014-12-23T16:12:54","profileUrl":"https://disqus.com/by/aimanmutasembellh/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"136724852","avatar":{"permalink":"https://disqus.com/api/users/avatars/aimanmutasembellh.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/aimanmutasembellh.jpg","cache":"https://c.disquscdn.com/uploads/users/13672/4852/avatar128.jpg?1589804941"},"cache":"https://c.disquscdn.com/uploads/users/13672/4852/avatar92.jpg?1589804941","large":{"permalink":"https://disqus.com/api/users/avatars/aimanmutasembellh.jpg","cache":"https://c.disquscdn.com/uploads/users/13672/4852/avatar92.jpg?1589804941"},"small":{"permalink":"https://disqus.com/api/users/avatars/aimanmutasembellh.jpg","cache":"https://c.disquscdn.com/uploads/users/13672/4852/avatar32.jpg?1589804941"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4227028789,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Hello Mr. Farzad. Kindlly, did you fix this issue? :)","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":1,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2021-05-20T18:21:28","dislikes":0,"thread":"6595120245","numReports":0,"likes":0,"message":"\u003cp>In case other people having the same issue. In run_epoch function, should use total_loss += float(loss) instead of total_loss += loss. Otherwise may have CUDA out of memory error. \u003cbr>Explanation see: \u003ca href=\"https://disq.us/url?url=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Fnotes%2Ffaq.html%3AvbhubPu_TN8sWtqWT1LX4hNEyoA&amp;cuid=5460729\" rel=\"nofollow noopener\" title=\"https://pytorch.org/docs/stable/notes/faq.html\">https://pytorch.org/docs/st...\u003c/a>\u003c/p>","id":"5382366618","createdAt":"2021-05-13T18:21:28","author":{"username":"disqus_gDirQgYlPU","about":"","name":"yulong li","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2021-04-22T15:11:34","profileUrl":"https://disqus.com/by/disqus_gDirQgYlPU/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"368182855","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_gDirQgYlPU.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_gDirQgYlPU.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_gDirQgYlPU.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_gDirQgYlPU.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4227028789,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"In case other people having the same issue. In run_epoch function, should use total_loss += float(loss) instead of total_loss += loss. Otherwise may have CUDA out of memory error. \nExplanation see: https://pytorch.org/docs/stable/notes/faq.html","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":1,"points":0,"moderationLabels":["links"],"isEdited":false,"sb":false},{"editableUntil":"2018-04-16T04:28:39","dislikes":0,"thread":"6595120245","numReports":0,"likes":1,"message":"\u003cp>Thanks!\u003c/p>","id":"3845808761","createdAt":"2018-04-09T04:28:39","author":{"username":"disqus_htlIvpMsai","about":"","name":"\uc8fc\ub9d0\uc774\ub2e4","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2016-02-01T03:44:02","profileUrl":"https://disqus.com/by/disqus_htlIvpMsai/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"194386225","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_htlIvpMsai.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_htlIvpMsai.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_htlIvpMsai.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_htlIvpMsai.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Thanks!","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":0,"points":1,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-05-03T14:09:08","dislikes":2,"thread":"6595120245","numReports":0,"likes":2,"message":"\u003cp>I have a question about class SublayerConnection.\u003cbr>the comment says \"for code simplicity the norm is first as opposed to last.\"\u003cbr>But the loss wont go down below 4~5 when the norm is applied after residual layer in the decoder, same as described in the paper.\u003cbr>Do you have any idea of this phenomenon?\u003c/p>","id":"3873740919","createdAt":"2018-04-26T14:09:08","author":{"username":"disqus_htlIvpMsai","about":"","name":"\uc8fc\ub9d0\uc774\ub2e4","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2016-02-01T03:44:02","profileUrl":"https://disqus.com/by/disqus_htlIvpMsai/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"194386225","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_htlIvpMsai.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_htlIvpMsai.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_htlIvpMsai.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_htlIvpMsai.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":null,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"I have a question about class SublayerConnection.\nthe comment says \"for code simplicity the norm is first as opposed to last.\"\nBut the loss wont go down below 4~5 when the norm is applied after residual layer in the decoder, same as described in the paper.\nDo you have any idea of this phenomenon?","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":0,"points":0,"moderationLabels":[],"isEdited":true,"sb":false},{"editableUntil":"2018-10-25T04:44:16","dislikes":0,"thread":"6595120245","numReports":0,"likes":5,"message":"\u003cp>same question. \u003cbr>but why not change the code into \u003cbr>return self.norm(x + self.dropout(sublayer(x)))\u003cbr>this seems more like the original paper?\u003c/p>","id":"4150550608","createdAt":"2018-10-18T04:44:16","author":{"username":"disqus_5iwwxhLeGL","about":"","name":"Yu-Tao","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2018-10-18T04:43:54","profileUrl":"https://disqus.com/by/disqus_5iwwxhLeGL/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"323780185","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_5iwwxhLeGL.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_5iwwxhLeGL.jpg","cache":"https://c.disquscdn.com/uploads/users/32378/185/avatar128.jpg?1539937069"},"cache":"https://c.disquscdn.com/uploads/users/32378/185/avatar92.jpg?1539937069","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_5iwwxhLeGL.jpg","cache":"https://c.disquscdn.com/uploads/users/32378/185/avatar92.jpg?1539937069"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_5iwwxhLeGL.jpg","cache":"https://c.disquscdn.com/uploads/users/32378/185/avatar32.jpg?1539937069"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3873740919,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"same question. \nbut why not change the code into \nreturn self.norm(x + self.dropout(sublayer(x)))\nthis seems more like the original paper?","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":1,"points":5,"moderationLabels":[],"isEdited":false,"sb":false},{"editableUntil":"2018-11-26T11:22:09","dislikes":0,"thread":"6595120245","numReports":0,"likes":6,"message":"\u003cp>Yey, I have the same question. And  There new code in (\u003ca href=\"https://disq.us/url?url=https%3A%2F%2Fgithub.com%2FOpenNMT%2FOpenNMT-py%2Fblob%2Fcd29c1dbfb35f4a2701ff52a1bf4e5bdcf02802e%2Fonmt%2Fencoders%2Ftransformer.py%23L48%29%3AkPEDwJxosNkT2LbhtOQLR49t7Oo&amp;cuid=5460729\" rel=\"nofollow noopener\" title=\"https://github.com/OpenNMT/OpenNMT-py/blob/cd29c1dbfb35f4a2701ff52a1bf4e5bdcf02802e/onmt/encoders/transformer.py#L48)\">https://github.com/OpenNMT/...\u003c/a> still writing like that. Obviously, the formula they list does not match the code they have.\u003c/p>","id":"4202213577","createdAt":"2018-11-19T11:22:09","author":{"username":"disqus_akZ5ASHeyH","about":"","name":"Neo li","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-07-09T05:34:51","profileUrl":"https://disqus.com/by/disqus_akZ5ASHeyH/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"258332319","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_akZ5ASHeyH.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_akZ5ASHeyH.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_akZ5ASHeyH.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_akZ5ASHeyH.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4150550608,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"Yey, I have the same question. And  There new code in (https://github.com/OpenNMT/OpenNMT-py/blob/cd29c1dbfb35f4a2701ff52a1bf4e5bdcf02802e/onmt/encoders/transformer.py#L48) still writing like that. Obviously, the formula they list does not match the code they have.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":2,"points":6,"moderationLabels":["links"],"isEdited":false,"sb":false},{"editableUntil":"2019-05-27T13:37:02","dislikes":0,"thread":"6595120245","numReports":0,"likes":1,"message":"\u003cp>It appears that the code provided by original the authors of the paper differs in this matter from what's stated in the paper.\u003cbr>Here, they seem to implement the original authors' code instead of the original paper.\u003c/p>\u003cp>Reference:\u003cbr>\u003ca href=\"https://disq.us/url?url=https%3A%2F%2Fgithub.com%2FOpenNMT%2FOpenNMT-py%2Fissues%2F770%23issuecomment-398299135%3A09ugKrHc-S2FOi-LYJRpzwGY2mA&amp;cuid=5460729\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://github.com/OpenNMT/OpenNMT-py/issues/770#issuecomment-398299135\">https://github.com/OpenNMT/...\u003c/a>\u003c/p>","id":"4469709216","createdAt":"2019-05-20T13:37:02","author":{"username":"prashantserai","about":"Electronics Engineer","name":"Prashant serai","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2014-04-30T05:37:32","profileUrl":"https://disqus.com/by/prashantserai/","url":"","location":"Mumbai","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"105178679","avatar":{"permalink":"https://disqus.com/api/users/avatars/prashantserai.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/prashantserai.jpg","cache":"https://c.disquscdn.com/uploads/users/10517/8679/avatar128.jpg?1558359423"},"cache":"https://c.disquscdn.com/uploads/users/10517/8679/avatar92.jpg?1558359423","large":{"permalink":"https://disqus.com/api/users/avatars/prashantserai.jpg","cache":"https://c.disquscdn.com/uploads/users/10517/8679/avatar92.jpg?1558359423"},"small":{"permalink":"https://disqus.com/api/users/avatars/prashantserai.jpg","cache":"https://c.disquscdn.com/uploads/users/10517/8679/avatar32.jpg?1558359423"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4150550608,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"It appears that the code provided by original the authors of the paper differs in this matter from what's stated in the paper.\nHere, they seem to implement the original authors' code instead of the original paper.\n\nReference:\nhttps://github.com/OpenNMT/OpenNMT-py/issues/770#issuecomment-398299135","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":2,"points":1,"moderationLabels":["links"],"isEdited":false,"sb":false},{"editableUntil":"2019-08-14T14:02:37","dislikes":0,"thread":"6595120245","numReports":0,"likes":0,"message":"\u003cp>I too was confused about this, but it seems they are true to the implementation in T2T (\u003ca href=\"https://disq.us/url?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensor2tensor%2Fblob%2F43be271c8a3fa06cb06b5147f044cbdc8bb77535%2Ftensor2tensor%2Fmodels%2Ftransformer.py%23L1799-L1800%3AlvQ3PVGJTkoDJWSy8faq-PLPCas&amp;cuid=5460729\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://github.com/tensorflow/tensor2tensor/blob/43be271c8a3fa06cb06b5147f044cbdc8bb77535/tensor2tensor/models/transformer.py#L1799-L1800\">https://github.com/tensorfl...\u003c/a> specifies the hyperparameters for the pre and post layer transformations applied by \u003ca href=\"https://disq.us/url?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensor2tensor%2Fblob%2F43be271c8a3fa06cb06b5147f044cbdc8bb77535%2Ftensor2tensor%2Flayers%2Fcommon_layers.py%23L862%29%3APcLTR_7HqSnJa4tSB9Ba8KMyPHc&amp;cuid=5460729\" rel=\"nofollow noopener\" target=\"_blank\" title=\"https://github.com/tensorflow/tensor2tensor/blob/43be271c8a3fa06cb06b5147f044cbdc8bb77535/tensor2tensor/layers/common_layers.py#L862)\">https://github.com/tensorfl...\u003c/a>. The description in the paper is inconsistent with the implementation.\u003c/p>","id":"4569337441","createdAt":"2019-08-07T14:02:37","author":{"username":"willprice94","about":"","name":"willprice94","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2012-01-21T12:40:38","profileUrl":"https://disqus.com/by/willprice94/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"21431364","avatar":{"permalink":"https://disqus.com/api/users/avatars/willprice94.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/willprice94.jpg","cache":"https://c.disquscdn.com/uploads/users/2143/1364/avatar128.jpg?1565186560"},"cache":"https://c.disquscdn.com/uploads/users/2143/1364/avatar92.jpg?1565186560","large":{"permalink":"https://disqus.com/api/users/avatars/willprice94.jpg","cache":"https://c.disquscdn.com/uploads/users/2143/1364/avatar92.jpg?1565186560"},"small":{"permalink":"https://disqus.com/api/users/avatars/willprice94.jpg","cache":"https://c.disquscdn.com/uploads/users/2143/1364/avatar32.jpg?1565186560"},"isCustom":true}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":4150550608,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"I too was confused about this, but it seems they are true to the implementation in T2T (https://github.com/tensorflow/tensor2tensor/blob/43be271c8a3fa06cb06b5147f044cbdc8bb77535/tensor2tensor/models/transformer.py#L1799-L1800 specifies the hyperparameters for the pre and post layer transformations applied by https://github.com/tensorflow/tensor2tensor/blob/43be271c8a3fa06cb06b5147f044cbdc8bb77535/tensor2tensor/layers/common_layers.py#L862). The description in the paper is inconsistent with the implementation.","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":2,"points":0,"moderationLabels":["links"],"isEdited":true,"sb":false},{"editableUntil":"2018-11-26T11:38:57","dislikes":1,"thread":"6595120245","numReports":0,"likes":2,"message":"\u003cp>\u003ca href=\"https://disq.us/url?url=https%3A%2F%2Fgithub.com%2FOpenNMT%2FOpenNMT-py%2Fissues%2F770%3AP4vM-d65dWitBlBZMRrqtd4pcFw&amp;cuid=5460729\" rel=\"nofollow noopener\" title=\"https://github.com/OpenNMT/OpenNMT-py/issues/770\">https://github.com/OpenNMT/...\u003c/a>\u003c/p>","id":"4202228717","createdAt":"2018-11-19T11:38:57","author":{"username":"disqus_akZ5ASHeyH","about":"","name":"Neo li","disable3rdPartyTrackers":false,"isPowerContributor":false,"joinedAt":"2017-07-09T05:34:51","profileUrl":"https://disqus.com/by/disqus_akZ5ASHeyH/","url":"","location":"","isPrivate":false,"signedUrl":"","isPrimary":true,"isAnonymous":false,"id":"258332319","avatar":{"permalink":"https://disqus.com/api/users/avatars/disqus_akZ5ASHeyH.jpg","xlarge":{"permalink":"https://disqus.com/api/users/avatars/disqus_akZ5ASHeyH.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar128.png"},"cache":"//a.disquscdn.com/1730296160/images/noavatar92.png","large":{"permalink":"https://disqus.com/api/users/avatars/disqus_akZ5ASHeyH.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar92.png"},"small":{"permalink":"https://disqus.com/api/users/avatars/disqus_akZ5ASHeyH.jpg","cache":"//a.disquscdn.com/1730296160/images/noavatar32.png"},"isCustom":false}},"media":[],"isSpam":false,"hasMore":false,"isDeleted":false,"isDeletedByAuthor":false,"parent":3873740919,"isApproved":true,"isNewUserNeedsApproval":false,"isFlagged":false,"raw_message":"https://github.com/OpenNMT/OpenNMT-py/issues/770","isAtFlagLimit":false,"isHighlighted":false,"canVote":false,"forum":"harvard-nlp","depth":1,"points":1,"moderationLabels":["links"],"isEdited":false,"sb":false}],"thread":{"feed":"https://harvard-nlp.disqus.com/harvardnlp_47/latest.rss","clean_title":"harvardnlp","dislikes":0,"likes":116,"message":"","ratingsEnabled":false,"isSpam":false,"isDeleted":false,"category":"7581793","adsDisabled":false,"author":"213053281","id":"6595120245","signedLink":"http://disq.us/?url=http%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&key=QXDYw8NbixHqzWxcbLJbfA","createdAt":"2018-04-04T16:38:21","hasStreaming":false,"raw_message":"","isClosed":false,"link":"http://nlp.seas.harvard.edu/2018/04/03/attention.html","slug":"harvardnlp_47","forum":"harvard-nlp","identifiers":[],"posts":138,"moderators":[213053281],"validateAllPosts":false,"title":"harvardnlp","highlightedPost":null}},"order":"popular"}</script>


    <div id="fixed-content"></div>

    
        <script type="text/javascript">
          var embedv2assets = window.document.createElement('script');
          embedv2assets.src = 'https://c.disquscdn.com/embedv2/latest/embedv2.js';
          embedv2assets.async = true;

          window.document.body.appendChild(embedv2assets);
        </script><script src="./embedv2.js.다운로드" async=""></script>
    



    
        
            
<script type="text/json" id="disqus-urls">{
    "root":"//disqus.com",
    "next":"https://c.disquscdn.com/next/current"
}</script>

        
        
        <script>!function () {
            var d = document;
            var toH = d.head.appendChild.bind(d.head);

            var v = window.location.href.match(/[#&?]version=([0-9a-f]{32})/);
            var src = 'https://c.disquscdn.com/next/embed/lounge.load';
            if (v)
                src += '.' + v[1];
            src += '.js';

            var s = d.createElement('script');
            s.crossOrigin = 'anonymous';
            s.id = 'bootstrap-script';
            s.setAttribute('data-app', 'lounge');
            s.src = src;
            toH(s);

            var m = d.createElement('meta');
            m.setAttribute('http-equiv', 'Content-Security-Policy');
            m.setAttribute('content', "script-src https:;");
            toH(m);
        }();</script>
    


<script src="./common.bundle.d02906baeab9c3cf3d19382f618732c6.js.다운로드"></script><div id="layout" data-tracking-area="layout"><div id="thread__container"><div><div id="thread__wrapper"><div id="placement-top" data-tracking-area="discovery-north"></div><div id="onboard" data-tracking-area="onboard"><div></div></div><div id="reactions__container"></div><div id="ratings__container"></div><div id="badges-message__container"></div><div id="global-alert"></div><div id="tos__container"></div><header id="main-nav" data-tracking-area="main-nav"><div><nav class="nav nav-primary nav-primary--refresh"><ul><li class="nav-tab nav-tab--primary tab-conversation tab-conversation--refresh active" data-role="post-count"><a class="publisher-nav-color"><span class="comment-count">138 comments</span></a></li><li class="nav-tab nav-tab--primary tab-user"><ul><li class="nav-tab nav-tab--primary notification-menu unread" data-role="notification-menu" style="display: list-item;"><a href="https://disqus.com/home/inbox/" class="notification-container" data-action="home" data-home-path="home/notifications/"><span class="notification-icon notification-icon--refresh icon-comment" aria-hidden="true"></span><span class="notification-count notification-count--refresh" data-role="notification-count">1</span></a></li><li class="nav-tab nav-tab--primary dropdown user-menu" data-role="logout"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="dropdown-toggle dropdown-toggle--refresh" data-toggle="dropdown" role="menuitem" name="Login"><span class="dropdown-toggle-wrapper"><span class="username username--refresh">Login</span> </span> <span class="caret caret--refresh"></span></a><ul class="dropdown-menu dropdown-menu--refresh"><li><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="auth:disqus">Disqus</a></li><li><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="auth:facebook">Facebook</a></li><li><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="auth:twitter">X (Twitter)</a></li><li><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="auth:google">Google</a></li><li><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="auth:microsoft">Microsoft</a></li><li><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="auth:apple">Apple</a></li></ul></li></ul></li></ul></nav></div></header><section id="conversation" data-role="main" data-tracking-area="main"><div id="posts"><div id="form" class="textarea-outer-wrapper--top-level"><form class="reply form-refresh form-refresh-v2"><div class="postbox"><div role="alert"></div><div class="ratings-wrapper" data-role="ratings-container"></div><div class="compose-wrapper"><div class="avatar"><span class="user user--refresh"><div>G</div></span></div><div class="textarea-outer-wrapper textarea-outer-wrapper--refresh"><div class="textarea-wrapper textarea-wrapper--embedv2" data-role="textarea" dir="auto"><div class="_container_ylcfx_1" role="presentation"><div class="_editor-container_ylcfx_37"><div class="_placeholder_s9avi_1">Join the discussion…</div><div role="textbox" aria-multiline="true" class="_editor_ylcfx_13" spellcheck="true" data-slate-editor="true" data-slate-node="value" contenteditable="true" zindex="-1" style="position: relative; white-space: pre-wrap; overflow-wrap: break-word;"><div data-slate-node="element"><span data-slate-node="text"><span data-slate-leaf="true"><span data-slate-zero-width="n" data-slate-length="0">﻿<br></span></span></span></div></div><button class="_placeholder-submit-button_ylcfx_55" aria-hidden="true" type="button">Comment</button></div><input accept="image/png,.png,image/jpeg,.jpeg,.jpg,image/gif,.gif" multiple="" type="file" tabindex="-1" style="display: none;"></div></div></div></div><div data-role="login-form"><div><div><section class="auth-section logged-out__display"><div class="connect"><h6 class="connect__heading">Log in with</h6><ul data-role="login-menu" class="services login-buttons"><li class="auth-disqus"><button type="button" data-action="auth:disqus" title="Disqus" class="connect__button" aria-label="Login with Disqus"><i class="icon-disqus"></i></button></li><li class="auth-facebook"><button type="button" data-action="auth:facebook" title="Facebook" class="connect__button" aria-label="Login with Facebook"><i class="icon-facebook-circle"></i></button></li><li class="auth-twitter"><button type="button" data-action="auth:twitter" title="X (Twitter)" class="connect__button" aria-label="Login with X (Twitter)"><i class="icon-twitter-x"></i></button></li><li class="auth-google"><button type="button" data-action="auth:google" title="Google" class="connect__button" aria-label="Login with Google"><i class="icon-google-plus-circle"></i></button></li><li class="auth-microsoft"><button type="button" data-action="auth:microsoft" title="Microsoft" class="connect__button" aria-label="Login with Microsoft"></button></li><li class="auth-apple"><button type="button" data-action="auth:apple" title="Apple" class="connect__button" aria-label="Login with Apple"></button></li></ul></div><div class="guest guest--refresh"><div class="sign-up-wrapper-refresh"><h6 class="guest-form-title guest-form-title--refresh"><span class="register-text"> or sign up with Disqus </span><span class="guest-text"> or pick a name </span></h6> <button type="button" class="help-tooltip__wrapper help-icon" name="guest_tooltip" tabindex="0"><div id="rules" class="help-tooltip__container" data-role="guest-form-tooltip"><div class="tooltip show help-tooltip"><h3 class="help-tooltip__heading">Disqus is a discussion network</h3><ul class="help-tooltip__list"><li><span>Don't be a jerk or do anything illegal. Everything is easier that way.</span></li></ul><p class="clearfix"><a href="https://docs.disqus.com/kb/terms-and-policies/" class="btn btn-small help-tooltip__button" rel="noopener noreferrer" target="_blank">Read full terms and conditions</a></p></div></div></button></div><p class="input-wrapper"><input dir="auto" type="text" placeholder="Name" name="display_name" id="view119_display_name" maxlength="30" class="input--text" aria-label="name"></p><div class="guest-details " data-role="guest-details"><p class="input-wrapper"><input dir="auto" type="email" placeholder="Email" name="email" id="view119_email" class="input--text" aria-label="email"></p><p class="input-wrapper"><input dir="auto" disabled="" type="text" class="register-text input--text" placeholder="Password" name="password" aria-label="password" id="view119_password"></p><p class="privacy-info"><div>By clicking submit, I authorize Disqus, Inc. and its affiliated companies to:</div><div><ul class="privacy-info-list"><li>Use, sell, and share my information to enable me to use its comment services and for marketing purposes, including cross-context behavioral advertising, as described in our <a href="https://help.disqus.com/customer/portal/articles/466260-terms-of-service" target="_blank" rel="noopener noreferrer">Terms of Service</a> and <a href="https://disqus.com/privacy-policy" target="_blank" rel="noopener noreferrer">Privacy Policy</a></li><li>Supplement the information that I provide with additional information lawfully obtained from other sources, like demographic data from public sources, interests inferred from web page views, or other data relevant to what might interest me, like past purchase or location data</li><li>Contact me or enable others to contact me by email with offers for goods and services (from any category) at the email address provided</li><li>Process any sensitive personal information that I submit in a comment for the purpose of displaying the comment</li><li>Retain my information while I am engaging with marketing messages that I receive and for a reasonable amount of time thereafter. I understand I can opt out at any time through an email that I receive.  Companies that we share data with are listed <a href="https://help.disqus.com/en/articles/1944034-cookies-and-data-recipients" target="_blank" rel="noopener noreferrer">here</a>.</li></ul></div></p><input type="checkbox" name="author-guest" style="display: none;"><div class="g-recaptcha" data-role="grecaptcha-container"></div><div class="proceed" data-role="submit-btn-container"><button type="submit" class="proceed__button btn submit" aria-label="Next"><span class="icon-proceed"></span><div class="spinner"></div></button></div></div></div></section></div></div></div></div></form></div><div id="email-signup"></div><div id="secondary-navigation"><div data-tracking-area="secondary-nav" class="nav-secondary-refresh"><ul class="nav-secondary-refresh__list"><li class="nav-secondary-refresh__list-item"><div id="favorite-button"><div class="thread-likes"><div><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="favorite" title="Favorite this discussion" class="favorite-button-toggle favorite-button-toggle--v2 " aria-label="Favorite this discussion"><span class="label label-default"><span class="favorite-icon favorite-icon--refresh-v2 icon-heart-empty"></span></span><span class="label label-favorited"><span class="favorite-icon favorite-icon--refresh-v2 icon-heart"></span></span> <span class="label label-count-refresh label-count-refresh--v2">116</span></a><ul class="dropdown-menu dropdown-menu--coachmark pull-right"><li><div><h2 class="coachmark__heading">Discussion Favorited!</h2><p class="coachmark__description">Favoriting means this is a discussion worth sharing. It gets shared to your followers' Disqus feeds, and gives the creator kudos!</p></div> <a href="https://disqus.com/home/?utm_source=disqus_embed&amp;utm_content=recommend_btn" class="btn btn-primary coachmark__button" target="_blank" rel="noopener noreferrer">Find More Discussions</a></li></ul></div></div></div><div id="thread-share-bar" class="share-bar-refresh"><div class="thread-share-wrapper"><div class="round-delimiter"></div><span data-role="thread-share" class="thread-share-bar-buttons-refresh"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="share-button-toggle share-button-toggle--v2" data-toggle="dropdown" aria-label="Share">Share</a><ul class="share-dropdown-refresh"><li class="share-dropdown-refresh__item"><div class="share-icons-wrapper share-icons-wrapper--twitter" data-action="share:twitter"><span class="icon-twitter-x" aria-hidden="true"></span><span class="visually-hidden">Tweet this discussion</span></div></li><li class="share-dropdown-refresh__item"><div class="share-icons-wrapper share-icons-wrapper--facebook" data-action="share:facebook"><span class="icon-facebook" aria-hidden="true"></span><span class="visually-hidden">Share this discussion on Facebook</span></div></li><li class="share-dropdown-refresh__item"><div class="share-icons-wrapper share-icons-wrapper--email" data-action="share:email"><span class="icon-mail" aria-hidden="true"></span><span class="visually-hidden">Share this discussion via email</span></div></li><li class="share-dropdown-refresh__item"><div class="share-icons-wrapper share-icons-wrapper--link" data-action="copy-link" title="Click to copy discussion link"><span class="icon-link" aria-hidden="true"></span><span class="visually-hidden">Copy link to discussion</span></div></li></ul></span></div></div></li><li data-role="post-sort" class=""><ul class="sort-menu-refresh"><li class="sort-menu-refresh__item selected"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="sort" data-sort="popular">Best</a></li><li class="sort-menu-refresh__item "><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="sort" data-sort="desc">Newest</a></li><li class="sort-menu-refresh__item "><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="sort" data-sort="asc">Oldest</a></li></ul></li></ul></div></div><div id="no-posts" style="display: none;"></div><div id="highlighted-post" data-tracking-area="highlighted" class="highlighted-post" style="display: none;"></div><button class="alert alert--realtime alert--realtime--refresh alert--realtime--refresh-v2" data-role="realtime-notification" style="display: none;"></button><ul id="post-list" class="post-list"><li class="post" id="post-4177145673"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4177145673"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/free_variation/" data-action="profile" data-tab="" data-username="free_variation" target="_blank" rel="noopener noreferrer" class="user user--refresh"><img data-role="user-avatar" data-user="54958964" src="./noavatar92.png" alt="Avatar" class="image-refresh"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/free_variation/" data-action="profile" data-tab="" data-username="free_variation" target="_blank" rel="noopener noreferrer">free_variation</a></span> <a data-action="follow" data-user="54958964" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4177145673" data-role="relative-time" class="time-ago" title="Sunday, November 4, 2018 6:56 AM">6 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thank you for a great piece.</p><p>I have a question about the positional encoding.  Here you seem to interleave sine and cosine curves over the dimensions for a given position.  The indexing in the original paper suggests the same, but the code at <a href="https://disq.us/url?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensor2tensor%2Fblob%2Fmaster%2Ftensor2tensor%2Flayers%2Fcommon_attention.py%3ACTNdLeTn1cJXk0PU4IiBnjHRR2A&amp;cuid=5460729" rel="nofollow noopener" target="_blank" title="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py">https://github.com/tensorfl...</a> (and the discussion at <a href="http://disq.us/url?url=http%3A%2F%2Fjalammar.github.io%2Fillustrated-transformer%2F%3AJ0pFreh0LdTMQXRXyIcFL5TyZUs&amp;cuid=5460729" rel="nofollow noopener" target="_blank" title="http://jalammar.github.io/illustrated-transformer/">http://jalammar.github.io/i...</a> ) suggests a concatenation of sines on the left and cosines on the right.</p><p>I'm not sure it makes a substantive difference, but would you agree there's a difference in implementation here?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-10" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">10</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-1" data-action="downvote" title="" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">1</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4177145673" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1x2yq89" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4177145673"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4992836748"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4992836748"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_vsvfcRoRBq/" data-action="profile" data-tab="" data-username="disqus_vsvfcRoRBq" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>P</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_vsvfcRoRBq/" data-action="profile" data-tab="" data-username="disqus_vsvfcRoRBq" target="_blank" rel="noopener noreferrer">Paul Nguyen</a></span> <a data-action="follow" data-user="352815913" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4177145673" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> free_variation</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4992836748" data-role="relative-time" class="time-ago" title="Wednesday, July 15, 2020 11:47 PM">4 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>free_variation did you ever find the answer to this?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">0</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4992836748" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/2akltho" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4992836748"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-6002455660"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="6002455660"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/mark_daoust/" data-action="profile" data-tab="" data-username="mark_daoust" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>M</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/mark_daoust/" data-action="profile" data-tab="" data-username="mark_daoust" target="_blank" rel="noopener noreferrer">Mark Daoust</a></span> <a data-action="follow" data-user="260458999" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4992836748" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Paul Nguyen</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-6002455660" data-role="relative-time" class="time-ago" title="Monday, October 3, 2022 10:06 PM">2 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Yes. This is easier to implement just concatenating them instead of interleaving them. It makes no difference unless you're trying to do something like load a checkpoint from another implementation.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">0</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-6002455660" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/2r9pgfg" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:6002455660"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-6002455660-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4992836748-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4177145673-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4885888969"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4885888969"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_wGGAF5yQKM/" data-action="profile" data-tab="" data-username="disqus_wGGAF5yQKM" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>G</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_wGGAF5yQKM/" data-action="profile" data-tab="" data-username="disqus_wGGAF5yQKM" target="_blank" rel="noopener noreferrer">Gus Smith</a></span> <a data-action="follow" data-user="177403162" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4885888969" data-role="relative-time" class="time-ago" title="Thursday, April 23, 2020 6:38 AM">5 years ago</a> <span> <span class="has-edit" data-role="has-edit">edited</span></span></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>The loss computation appears to be updating gradients during validation. <br>This appears to drive weight updates to fit the validation (test) data, which it should not.</p><p>A recommended way to run validation is apparently: <br><code>model.eval()<br>torch.no_grad()</code><br>[<a href="https://disq.us/url?url=https%3A%2F%2Fdiscuss.pytorch.org%2Ft%2Fmodel-eval-vs-with-torch-no-grad%2F19615%3Ae_08I4EBqohG3_nIbsY1i9V_SvA&amp;cuid=5460729" rel="nofollow noopener" target="_blank" title="https://discuss.pytorch.org/t/model-eval-vs-with-torch-no-grad/19615">https://discuss.pytorch.org...</a>](url)<br><a href="https://disq.us/url?url=https%3A%2F%2Fdiscuss.pytorch.org%2Ft%2Ftwo-small-questions-about-with-torch-no-grad%2F27571%3AzewGg_fpEMy6QbmM9X4Lh6ovyRk&amp;cuid=5460729" rel="nofollow noopener" target="_blank" title="https://discuss.pytorch.org/t/two-small-questions-about-with-torch-no-grad/27571">https://discuss.pytorch.org...</a></p><p>And indeed, the code that runs without error is the following, where loss.backward()<br>is pushed under the if statement:</p><p></p><pre><code>        <br>        model.eval()<br>        with torch.no_grad():<br>            test_loss = run_epoch(val_dataloader, model, <br>                  SimpleLossCompute(model.generator, criterion, None))<br>            print("test_loss", test_loss)<br></code></pre><p><br>and: <br></p><pre><code><br>class SimpleLossCompute:<br>    "A simple loss compute and train function."<br>    def __init__(self, generator, criterion, opt=None):<br>        self.generator = generator<br>        self.criterion = criterion<br>        self.opt = opt<br>        <br>    def __call__(self, x, y, norm):<br>        x = self.generator(x)<br>        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), <br>                              y.contiguous().view(-1)) / norm<br>        #loss.backward()<br>        if self.opt is not None:<br>            loss.backward()<br>            self.opt.step()<br>            self.opt.optimizer.zero_grad()<br>        return loss.data * norm</code></pre><p></p><p>This was discovered in another forum.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-5" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">5</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4885888969" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/28sxk21" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4885888969"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4986690707"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4986690707"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_vsvfcRoRBq/" data-action="profile" data-tab="" data-username="disqus_vsvfcRoRBq" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>P</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_vsvfcRoRBq/" data-action="profile" data-tab="" data-username="disqus_vsvfcRoRBq" target="_blank" rel="noopener noreferrer">Paul Nguyen</a></span> <a data-action="follow" data-user="352815913" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4885888969" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Gus Smith</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4986690707" data-role="relative-time" class="time-ago" title="Friday, July 10, 2020 10:25 PM">4 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Hey Gus, did you end up getting this code to work? Did you want to compare results?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">0</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4986690707" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/2agy36b" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4986690707"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4986690707-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4885888969-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4015929276"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4015929276"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/kaustubhkunte/" data-action="profile" data-tab="" data-username="kaustubhkunte" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>K</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/kaustubhkunte/" data-action="profile" data-tab="" data-username="kaustubhkunte" target="_blank" rel="noopener noreferrer">Kaustubh Kunte</a></span> <a data-action="follow" data-user="252947050" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4015929276" data-role="relative-time" class="time-ago" title="Wednesday, August 1, 2018 11:09 AM">6 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Question: Why does MultiHeadAttention class create only 4 clones instead of 8?<br>        self.linears = clones(nn.Linear(d_model, d_model), 4)</p><p>Thanks for a great post !</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-7" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">7</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-1" data-action="downvote" title="" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">1</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4015929276" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1uezav0" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4015929276"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4150577140"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4150577140"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_5iwwxhLeGL/" data-action="profile" data-tab="" data-username="disqus_5iwwxhLeGL" target="_blank" rel="noopener noreferrer" class="user user--refresh"><img data-role="user-avatar" data-user="323780185" src="./noavatar92.png" alt="Avatar" class="image-refresh"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_5iwwxhLeGL/" data-action="profile" data-tab="" data-username="disqus_5iwwxhLeGL" target="_blank" rel="noopener noreferrer">Yu-Tao</a></span> <a data-action="follow" data-user="323780185" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4015929276" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Kaustubh Kunte</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4150577140" data-role="relative-time" class="time-ago" title="Thursday, October 18, 2018 2:21 PM">6 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>I guess you misunderstand the zip function.<br>query, key, value = \<br>            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)<br>             for l, x in zip(self.linears, (query, key, value))]<br>In this code, the first linear layer works on query, the second one works on key and the third one works on value. The fourth one is used in: return self.linears[-1](x)<br>So the total number of linear layers is four.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-11" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">11</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4150577140" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1wn59tg" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4150577140"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4347070890"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4347070890"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/nimning/" data-action="profile" data-tab="" data-username="nimning" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>N</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/nimning/" data-action="profile" data-tab="" data-username="nimning" target="_blank" rel="noopener noreferrer">nimning</a></span> <a data-action="follow" data-user="329113907" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4150577140" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Yu-Tao</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4347070890" data-role="relative-time" class="time-ago" title="Thursday, February 21, 2019 8:20 AM">6 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Hi <br>Why the linear layer has dimension (d_model, d_model) ? The  W^Q, W^K, and W^V have dimension (d_model, d_k), (d_model, d_k), and (d_model, d_v)? I do not see any layer corresponding to any of the three matrices.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">1</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4347070890" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1zw4td6" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4347070890"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4359258506"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4359258506"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/yipuding/" data-action="profile" data-tab="" data-username="yipuding" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>Y</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/yipuding/" data-action="profile" data-tab="" data-username="yipuding" target="_blank" rel="noopener noreferrer">Yipu Ding</a></span> <a data-action="follow" data-user="329461722" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4347070890" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> nimning</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4359258506" data-role="relative-time" class="time-ago" title="Friday, March 1, 2019 2:14 PM">6 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>This is my understanding,<br>see the code: <br>query, key, value = \</p><p>            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linears, (query, key, value))]<br>this will let the query, key,value pass through the first three Linear layers in self.linears, <br>so the first three Linear layers will be the Wq,Wk and Wv as you mentioned.<br>after this the matrix of query,key and value will have the shape(num_batches, time_steps, d_model).</p><p>Now since d_k = d_model//h, by reshaping it to(num_batches, -1, h, d_k) will give you the same amount of data.(here, d_k = d_v = d_w)<br> You can think of this as taking the first d_v features from d_model as the first head, then d_v+1 to 2*d_v as the second head. So now we do have 8 heads just by splitting the features of size d_model that we learnt from the matrix.</p><p>Then by doing this: transpose(1, 2), you will get tensor of shape (n_batches, h, time steps, d_k), and by feeding query, key and value to self_attention function, you will get back a tensor of shape(n_batches, h, time_steps, d_k).</p><p>you can see now that by doing transpose(1,2) and reshaping it to (n_batches, -1, d_k*h)<br>will get you a tensor with the same shape as previous layer.</p><p>So the main idea is that instead of creating 8*3 matrices for all values, keys and queries,<br>we only create 3 bigger ones and then split it to get the smaller ones.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-4" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">4</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4359258506" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/203e1e2" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4359258506"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4487791589"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4487791589"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/francoissteiner/" data-action="profile" data-tab="" data-username="francoissteiner" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>F</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/francoissteiner/" data-action="profile" data-tab="" data-username="francoissteiner" target="_blank" rel="noopener noreferrer">Francois Steiner</a></span> <a data-action="follow" data-user="333338486" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4359258506" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Yipu Ding</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4487791589" data-role="relative-time" class="time-ago" title="Tuesday, June 4, 2019 1:57 AM">5 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks for a great post! <br>It seems to me that instead of splitting the original matrix in 8 for the multi-head attention, we should be creating 8 independent learnt projections, so the code should be re-written accordingly - any thoughts?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">0</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4487791589" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/227wy5h" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4487791589"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4628704871"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4628704871"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_KMuier91G9/" data-action="profile" data-tab="" data-username="disqus_KMuier91G9" target="_blank" rel="noopener noreferrer" class="user user--refresh"><img data-role="user-avatar" data-user="210307358" src="./noavatar92.png" alt="Avatar" class="image-refresh"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_KMuier91G9/" data-action="profile" data-tab="" data-username="disqus_KMuier91G9" target="_blank" rel="noopener noreferrer">Sidney Melo</a></span> <a data-action="follow" data-user="210307358" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4487791589" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Francois Steiner</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4628704871" data-role="relative-time" class="time-ago" title="Thursday, September 26, 2019 12:39 AM">5 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>I also believed the input would be projected entirely in 8 new vectors when I read the paper. Question is: to create 8 independent learnt projections is really equivalent to splitting the original matrix in 8?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">0</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4628704871" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/24jt7jb" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4628704871"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4645489194"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4645489194"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/chuankangwu/" data-action="profile" data-tab="" data-username="chuankangwu" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>C</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/chuankangwu/" data-action="profile" data-tab="" data-username="chuankangwu" target="_blank" rel="noopener noreferrer">chuankang wu</a></span> <a data-action="follow" data-user="338843326" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4628704871" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Sidney Melo</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4645489194" data-role="relative-time" class="time-ago" title="Wednesday, October 9, 2019 11:49 AM">5 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>it is equivalent， an example below：<br> <span></span><div class="media-container media-mode-deferred">
<a class="media-button media-button-expand publisher-color publisher-border-color" href="https://uploads.disquscdn.com/images/958f686f492ff60d3d74ff4f735fec3a7910508849569a4edf0457471237ea54.jpg" rel="nofollow" target="_blank" data-action="expand" title="">
<i class="icon-images publisher-background-color"></i>
View
</a>
<a class="media-button media-button-contract publisher-color publisher-border-color" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" target="_blank" data-action="contract">
<i class="icon-cancel publisher-background-color"></i> Hide
</a>
<div class="media-content-loader" data-role="content-loader"></div>
<div data-role="content-placeholder" class="media-content-placeholder media-Disquscdn " style="height: 1066px;"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="media-force-load" data-action="force-load"><i class="icon-images"></i></a>
</div>
</div></p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-9" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">9</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4645489194" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/24tsyei" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4645489194"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4645489194-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4628704871-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4487791589-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4359258506-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4347070890-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-5311741227"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="5311741227"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_vHGFqHGNX9/" data-action="profile" data-tab="" data-username="disqus_vHGFqHGNX9" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>海</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_vHGFqHGNX9/" data-action="profile" data-tab="" data-username="disqus_vHGFqHGNX9" target="_blank" rel="noopener noreferrer">海尔</a></span> <a data-action="follow" data-user="342134363" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4150577140" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Yu-Tao</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-5311741227" data-role="relative-time" class="time-ago" title="Sunday, March 21, 2021 2:12 PM">4 years ago</a> <span> <span class="has-edit" data-role="has-edit">edited</span></span></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks for your answer！</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">0</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-5311741227" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/2fuh1rf" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:5311741227"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-5311741227-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4150577140-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4015929276-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4547747695"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4547747695"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_8lgrEHvH9y/" data-action="profile" data-tab="" data-username="disqus_8lgrEHvH9y" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>F</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_8lgrEHvH9y/" data-action="profile" data-tab="" data-username="disqus_8lgrEHvH9y" target="_blank" rel="noopener noreferrer">Frank Wang</a></span> <a data-action="follow" data-user="335232778" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4547747695" data-role="relative-time" class="time-ago" title="Sunday, July 21, 2019 4:51 PM">5 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>For the `make_std_mask` method:<br>`tgt_mask` have the shape of (1, batches, length), and the `subsequent_mask` method should output shape of (1, length, length). How the two tensors have `&amp;` operator?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-3" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">3</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4547747695" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/237m0kv" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4547747695"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4840517822"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4840517822"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/kenenbekarzymatov/" data-action="profile" data-tab="" data-username="kenenbekarzymatov" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>K</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/kenenbekarzymatov/" data-action="profile" data-tab="" data-username="kenenbekarzymatov" target="_blank" rel="noopener noreferrer">Kenenbek Arzymatov</a></span> <a data-action="follow" data-user="149510443" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4547747695" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Frank Wang</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4840517822" data-role="relative-time" class="time-ago" title="Friday, March 20, 2020 11:14 PM">5 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Have you found an answer?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">0</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4840517822" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/281x3ge" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4840517822"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4994173476"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4994173476"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/vishwaaschandan/" data-action="profile" data-tab="" data-username="vishwaaschandan" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>V</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/vishwaaschandan/" data-action="profile" data-tab="" data-username="vishwaaschandan" target="_blank" rel="noopener noreferrer">vishwaas chandan</a></span> <a data-action="follow" data-user="176744344" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4840517822" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Kenenbek Arzymatov</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4994173476" data-role="relative-time" class="time-ago" title="Friday, July 17, 2020 12:07 AM">4 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>I was stuck in the same question, but then I ran the the Batch class with a sample data, turns out the `tgt_mask` after unsqueeze has size: (nbatch,1, length) and the `subsequent_mask` method should output shape is (1, length, length), and it looks like the pytorch "&amp;" operation (unlike tensorflow) automatically tiles the first tensor along 0 dimension and the second tensor along 1st dimension and gives an output of size: (nbatch, length, length), this is intuitive since we want to have masking on every row in the batch.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">1</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4994173476" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/2alegx0" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4994173476"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4995122656"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4995122656"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_vsvfcRoRBq/" data-action="profile" data-tab="" data-username="disqus_vsvfcRoRBq" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>P</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_vsvfcRoRBq/" data-action="profile" data-tab="" data-username="disqus_vsvfcRoRBq" target="_blank" rel="noopener noreferrer">Paul Nguyen</a></span> <a data-action="follow" data-user="352815913" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4994173476" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> vishwaas chandan</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4995122656" data-role="relative-time" class="time-ago" title="Friday, July 17, 2020 4:50 PM">4 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Could you try out different inputs for the "copy and paste" task? <br>src = Variable(torch.LongTensor([[1,2,3,4,5,6,7,8,9,10]])) I have been getting different outputs different to the inputs. Which is really strange since my translation task works just fine..... here my version of the code for pytorch 1.5.1 <a href="https://disq.us/url?url=https%3A%2F%2Fgithub.com%2Fmathematicsofpaul%2Ftransformer-update%3AixOcvBXLT-RexD2XT0QyvxC_lTk&amp;cuid=5460729" rel="nofollow noopener" target="_blank" title="https://github.com/mathematicsofpaul/transformer-update">https://github.com/mathemat...</a></p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">1</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4995122656" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/2alytb4" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4995122656"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4995122656-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4994173476-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4840517822-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4547747695-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4007049675"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4007049675"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/gaziev/" data-action="profile" data-tab="" data-username="gaziev" target="_blank" rel="noopener noreferrer" class="user user--refresh"><img data-role="user-avatar" data-user="23962751" src="./noavatar92.png" alt="Avatar" class="image-refresh"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/gaziev/" data-action="profile" data-tab="" data-username="gaziev" target="_blank" rel="noopener noreferrer">gaziev</a></span> <a data-action="follow" data-user="23962751" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4007049675" data-role="relative-time" class="time-ago" title="Thursday, July 26, 2018 7:24 PM">6 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks for great post!<br>Q: shouldn't it be mask.dim() &gt; 1 in LabelSmoothing#forward?<br>Because in your example with target [1] mask after nonzero([1] == 0) will be tensor([]) which has dimension 1 and it will throw error if we pass it as argument in index_fill_ as index.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-5" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">5</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-1" data-action="downvote" title="" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">1</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4007049675" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1u9ozbf" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4007049675"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4199309870"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4199309870"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/lipiji1986/" data-action="profile" data-tab="" data-username="lipiji1986" target="_blank" rel="noopener noreferrer" class="user user--refresh"><img data-role="user-avatar" data-user="34038110" src="./noavatar92.png" alt="Avatar" class="image-refresh"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/lipiji1986/" data-action="profile" data-tab="" data-username="lipiji1986" target="_blank" rel="noopener noreferrer">Piji Li</a></span> <a data-action="follow" data-user="34038110" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4007049675" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> gaziev</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4199309870" data-role="relative-time" class="time-ago" title="Saturday, November 17, 2018 4:27 PM">6 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>mask.sum() &gt; 0 ？</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-6" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">6</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-1" data-action="downvote" title="" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">1</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4199309870" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1xg5s8e" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4199309870"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4203482166"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4203482166"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_W48rgB1BxP/" data-action="profile" data-tab="" data-username="disqus_W48rgB1BxP" target="_blank" rel="noopener noreferrer" class="user user--refresh"><img data-role="user-avatar" data-user="325338843" src="./noavatar92.png" alt="Avatar" class="image-refresh"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_W48rgB1BxP/" data-action="profile" data-tab="" data-username="disqus_W48rgB1BxP" target="_blank" rel="noopener noreferrer">Ke Wang</a></span> <a data-action="follow" data-user="325338843" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4199309870" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Piji Li</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4203482166" data-role="relative-time" class="time-ago" title="Tuesday, November 20, 2018 12:02 PM">6 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>change it to "mask.sum() &gt; 0 and len(mask) &gt; 0", it works!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">0</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-1" data-action="downvote" title="" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">1</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4203482166" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1xin7li" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4203482166"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4203482166-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4199309870-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4152328935"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4152328935"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_5iwwxhLeGL/" data-action="profile" data-tab="" data-username="disqus_5iwwxhLeGL" target="_blank" rel="noopener noreferrer" class="user user--refresh"><img data-role="user-avatar" data-user="323780185" src="./noavatar92.png" alt="Avatar" class="image-refresh"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_5iwwxhLeGL/" data-action="profile" data-tab="" data-username="disqus_5iwwxhLeGL" target="_blank" rel="noopener noreferrer">Yu-Tao</a></span> <a data-action="follow" data-user="323780185" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4007049675" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> gaziev</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4152328935" data-role="relative-time" class="time-ago" title="Friday, October 19, 2018 5:17 PM">6 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Same problem. I solve it by replace MyIterator with data.BucketIterator.<br>I'm not sure if it is related with the batch data.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">0</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4152328935" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1wo6tif" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4152328935"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4152328935-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4007049675-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4349056928"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4349056928"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_pcWVloYWqZ/" data-action="profile" data-tab="" data-username="disqus_pcWVloYWqZ" target="_blank" rel="noopener noreferrer" class="user user--refresh"><img data-role="user-avatar" data-user="182623577" src="./noavatar92.png" alt="Avatar" class="image-refresh"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_pcWVloYWqZ/" data-action="profile" data-tab="" data-username="disqus_pcWVloYWqZ" target="_blank" rel="noopener noreferrer">Ning Ma</a></span> <a data-action="follow" data-user="182623577" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4349056928" data-role="relative-time" class="time-ago" title="Friday, February 22, 2019 5:16 PM">6 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>The output of each step in the decoder is fed to the bottom decoder in the next time step. So, based on thsi implementation, how can the decoder generate word one by one based on previous prediction? I do not see this logic in the code.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-4" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">4</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-1" data-action="downvote" title="" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">1</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4349056928" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1zxbdsw" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4349056928"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4510068543"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4510068543"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_yCoVzKJPyj/" data-action="profile" data-tab="" data-username="disqus_yCoVzKJPyj" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>I</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_yCoVzKJPyj/" data-action="profile" data-tab="" data-username="disqus_yCoVzKJPyj" target="_blank" rel="noopener noreferrer">Ishaan</a></span> <a data-action="follow" data-user="334049366" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4349056928" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Ning Ma</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4510068543" data-role="relative-time" class="time-ago" title="Friday, June 21, 2019 6:29 AM">5 years ago</a> <span> <span class="has-edit" data-role="has-edit">edited</span></span></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>During training, we don't need to feed the predicted word to the bottom decoder. We use teacher forcing here and feed the true word in the next time step. During testing though, we have to feed the predicted word (till &lt;eos&gt;) to generate the sentence. Please take a look towards the end here: <a href="https://disq.us/url?url=https%3A%2F%2Ftowardsdatascience.com%2Fhow-to-code-the-transformer-in-pytorch-24db27c8f9ec%3AbvDLL3adJKBQmU5c0DdyhSCRu2k&amp;cuid=5460729" rel="nofollow noopener" target="_blank" title="https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec">https://towardsdatascience....</a></p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">1</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4510068543" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/22l6f5r" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4510068543"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4510068543-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4349056928-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4416166916"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4416166916"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_T9X568BHlO/" data-action="profile" data-tab="" data-username="disqus_T9X568BHlO" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>A</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_T9X568BHlO/" data-action="profile" data-tab="" data-username="disqus_T9X568BHlO" target="_blank" rel="noopener noreferrer">Arthur Marques</a></span> <a data-action="follow" data-user="260788705" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4416166916" data-role="relative-time" class="time-ago" title="Wednesday, April 10, 2019 3:51 AM">6 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>In PositionalEncoding, there is a small fix discussed here: <a href="https://disq.us/url?url=https%3A%2F%2Fstackoverflow.com%2Fquestions%2F52922445%2Fruntimeerror-exp-not-implemented-for-torch-longtensor%3ABrNPJZE-73zYv0I0pJ8j2ZPvDEM&amp;cuid=5460729" rel="nofollow noopener" target="_blank" title="https://stackoverflow.com/questions/52922445/runtimeerror-exp-not-implemented-for-torch-longtensor">https://stackoverflow.com/q...</a></p><p>LU Jialin quote:</p><p>For me I just got the torch.arange to generate float type tensor</p><p>from</p><p>position = torch.arange(0, max_len).unsqueeze(1)<br>div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))<br>to</p><p>position = torch.arange(<b>0.</b>, max_len).unsqueeze(1)<br>div_term = torch.exp(torch.arange(<b>0.</b>, d_model, 2) * -(math.log(10000.0) / d_model))</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-2" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">2</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4416166916" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/2119s78" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4416166916"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4416166916-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4286765854"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4286765854"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_Wv15C2dVTm/" data-action="profile" data-tab="" data-username="disqus_Wv15C2dVTm" target="_blank" rel="noopener noreferrer" class="user user--refresh"><img data-role="user-avatar" data-user="94329578" src="./noavatar92.png" alt="Avatar" class="image-refresh"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_Wv15C2dVTm/" data-action="profile" data-tab="" data-username="disqus_Wv15C2dVTm" target="_blank" rel="noopener noreferrer">Иван</a></span> <a data-action="follow" data-user="94329578" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4286765854" data-role="relative-time" class="time-ago" title="Tuesday, January 15, 2019 5:36 PM">6 years ago</a> <span> <span class="has-edit" data-role="has-edit">edited</span></span></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks a lot for such great post!</p><p>I guess there is a mistake in following part of code:<br><code><br>def clones(module, N):<br>         "Produce N identical layers."<br>         return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])<br></code></p><p>"Deepcopy" copies the entire layer with it's weights what is wrong because (Wq_1, Wq_2, Wq_3, ..., Wk_1, Wk_2, Wk_3, ..., etc) must be randomly initialized. In other words "deepcopy" copies the only instance of class nn.Linear but N instances must be created. So I suppose that it's better to replace it by something like that:<br><code><br>def clones(params, N):<br>     return nn.ModuleList([nn.Linear(params) for _ in range(N)])<br></code></p><p>Sincerely, <br>Ivan</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-2" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">2</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4286765854" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1yw89pa" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4286765854"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4349244276"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4349244276"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/peixiangzhong/" data-action="profile" data-tab="" data-username="peixiangzhong" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>P</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/peixiangzhong/" data-action="profile" data-tab="" data-username="peixiangzhong" target="_blank" rel="noopener noreferrer">Peixiang Zhong</a></span> <a data-action="follow" data-user="202886301" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4286765854" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Иван</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4349244276" data-role="relative-time" class="time-ago" title="Friday, February 22, 2019 9:33 PM">6 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Later there is code for model initialization at line 20 in the notebook</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">0</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4349244276" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1zxfed0" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4349244276"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4349244276-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4286765854-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-5787940603"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="5787940603"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/runqiyang/" data-action="profile" data-tab="" data-username="runqiyang" target="_blank" rel="noopener noreferrer" class="user user--refresh"><img data-role="user-avatar" data-user="252521167" src="./noavatar92.png" alt="Avatar" class="image-refresh"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/runqiyang/" data-action="profile" data-tab="" data-username="runqiyang" target="_blank" rel="noopener noreferrer">Runqi Yang</a></span> <a data-action="follow" data-user="252521167" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-5787940603" data-role="relative-time" class="time-ago" title="Sunday, March 13, 2022 6:32 PM">3 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>The implementation of Embedding seems incorrect. <br><code>return self.lut(x) * math.sqrt(self.d_model)</code><br>The original paper mentioned multiplying by "sqrt(d_model)" because in Tensorflow implementation, the embedding weights are initialized to the inverse of sqrt(d_model), so they scale it back to N(0, 1). But in pytorch the initial embedding weights are already N(0,1), so multiplying by sqrt(d_model) will make the weights unreasonably large.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">1</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-5787940603" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/2npznjv" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:5787940603"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-5787940603-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-5725761556"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="5725761556"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/jimeverest/" data-action="profile" data-tab="" data-username="jimeverest" target="_blank" rel="noopener noreferrer" class="user user--refresh"><img data-role="user-avatar" data-user="381392369" src="./noavatar92.png" alt="Avatar" class="image-refresh"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/jimeverest/" data-action="profile" data-tab="" data-username="jimeverest" target="_blank" rel="noopener noreferrer">Jim Everest</a></span> <a data-action="follow" data-user="381392369" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-5725761556" data-role="relative-time" class="time-ago" title="Tuesday, February 8, 2022 12:16 PM">3 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks for this great post.</p><p>I still have some doubts about the Mask, I know it is to prevent the subsequent positions from being seen during model training. But why does the Mask mechanism only appear in Decoder’s self Attention? Shouldn’t the same mask mechanism be introduced in other modules of Decoder?</p><p>Before the first Multi-Headed Attention module in Decoder, there is also a residual structure, which will bring the original Output Embedding matrix(with entire output groung truth tokens) into the subsequent “ADD &amp; Norm” module, add this Masked matrix, and then perform following processing.</p><p>I think this will cause all GT Output tokens to be brought in in the subsequent processing, and will be passed all the way to the final output of the Decoder. Will this not lead to the leakage of the Ground Truth in the Linear and Softmax output stages?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">1</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-5725761556" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/2moyxw4" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:5725761556"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-5725761556-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-5349873650"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="5349873650"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/gilles_jack/" data-action="profile" data-tab="" data-username="gilles_jack" target="_blank" rel="noopener noreferrer" class="user user--refresh"><img data-role="user-avatar" data-user="102092859" src="./noavatar92.png" alt="Avatar" class="image-refresh"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/gilles_jack/" data-action="profile" data-tab="" data-username="gilles_jack" target="_blank" rel="noopener noreferrer">Gilles</a></span> <a data-action="follow" data-user="102092859" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-5349873650" data-role="relative-time" class="time-ago" title="Monday, April 19, 2021 3:09 AM">4 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>When doing the datasets.IWSLT.splits,  I have the following error : OSError: Not a gzipped file (b'</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">1</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-5349873650" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/2gh6cxe" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:5349873650"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-5450034176"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="5450034176"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/christophwindheuser/" data-action="profile" data-tab="" data-username="christophwindheuser" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>C</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/christophwindheuser/" data-action="profile" data-tab="" data-username="christophwindheuser" target="_blank" rel="noopener noreferrer">Christoph Windheuser</a></span> <a data-action="follow" data-user="236438827" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-5349873650" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Gilles</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-5450034176" data-role="relative-time" class="time-ago" title="Saturday, July 10, 2021 6:10 AM">3 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Same issue for me. It seems that the functionality of torchtext 0.10.0 has changed without backward compatibility and the code in this post has to be adapted to load the IWSLT database. Has someone figured out how to solve this? Please share. Thanks!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">0</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-5450034176" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/2i4t5a8" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:5450034176"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-5450034176-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-5366533776"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="5366533776"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/amdeworkasefa/" data-action="profile" data-tab="" data-username="amdeworkasefa" target="_blank" rel="noopener noreferrer" class="user user--refresh"><img data-role="user-avatar" data-user="368669535" src="./noavatar92.png" alt="Avatar" class="image-refresh"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/amdeworkasefa/" data-action="profile" data-tab="" data-username="amdeworkasefa" target="_blank" rel="noopener noreferrer">Amdework Asefa Zemaryam</a></span> <a data-action="follow" data-user="368669535" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-5349873650" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Gilles</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-5366533776" data-role="relative-time" class="time-ago" title="Saturday, May 1, 2021 5:38 PM">4 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Same issue, have you resolved it please? Again I was trying to run with local datasets but I can't split with torch as specified as in the code. <br> (train, val, test = datasets.IWSLT.splits(<br>    exts=('.en', '.de'), fields=(SRC, TGT), <br>    filter_pred=lambda x: len(vars(x)['src']) &lt;= MAX_LEN <br>    and len(vars(x)['trg']) &lt;= MAX_LEN))<br>How can I use this fragment for a local dataset please help.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">0</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-5366533776" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/2gr3fyo" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:5366533776"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-5366533776-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-5350335350"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="5350335350"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/ybli/" data-action="profile" data-tab="" data-username="ybli" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>Y</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/ybli/" data-action="profile" data-tab="" data-username="ybli" target="_blank" rel="noopener noreferrer">yb li</a></span> <a data-action="follow" data-user="367985524" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-5349873650" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Gilles</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-5350335350" data-role="relative-time" class="time-ago" title="Monday, April 19, 2021 12:14 PM">4 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>I have the same question as yours, do you figure it out</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">0</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-5350335350" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/2ghg96e" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:5350335350"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-5366537777"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="5366537777"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/amdeworkasefa/" data-action="profile" data-tab="" data-username="amdeworkasefa" target="_blank" rel="noopener noreferrer" class="user user--refresh"><img data-role="user-avatar" data-user="368669535" src="./noavatar92.png" alt="Avatar" class="image-refresh"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/amdeworkasefa/" data-action="profile" data-tab="" data-username="amdeworkasefa" target="_blank" rel="noopener noreferrer">Amdework Asefa Zemaryam</a></span> <a data-action="follow" data-user="368669535" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-5350335350" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> yb li</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-5366537777" data-role="relative-time" class="time-ago" title="Saturday, May 1, 2021 5:46 PM">4 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>not yet, as soon as you got it. please share us! thank you!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">0</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-5366537777" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/2gr3j1t" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:5366537777"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-5450509664"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="5450509664"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_RN5nYClnUa/" data-action="profile" data-tab="" data-username="disqus_RN5nYClnUa" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>B</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_RN5nYClnUa/" data-action="profile" data-tab="" data-username="disqus_RN5nYClnUa" target="_blank" rel="noopener noreferrer">Benjamin</a></span> <a data-action="follow" data-user="372134232" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-5366537777" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Amdework Asefa Zemaryam</a></span></span>  <span class="post-meta" style="top: -2px;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-5450509664" data-role="relative-time" class="time-ago" title="Saturday, July 10, 2021 6:10 PM">3 years ago</a> <span> <span class="has-edit" data-role="has-edit">edited</span></span></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>I solved this problem, just go to this website: <a href="https://disq.us/url?url=https%3A%2F%2Fwit3.fbk.eu%2F2016-01%3ACBSIOv8E4K-F33qrmUDVnUngo-o&amp;cuid=5460729" rel="nofollow noopener" target="_blank" title="https://wit3.fbk.eu/2016-01">https://wit3.fbk.eu/2016-01</a>, click "link" and download the "2016-01.tgz" file. In this file you can find the "2016-01/texts/de/en/de-en.tgz" file. Simply put this file in "your_work_dir/.data/iwslt/", then it works fine.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">1</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-5450509664" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/2i53c68" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:5450509664"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-5611852775"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="5611852775"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/stangerdanger/" data-action="profile" data-tab="" data-username="stangerdanger" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>S</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/stangerdanger/" data-action="profile" data-tab="" data-username="stangerdanger" target="_blank" rel="noopener noreferrer">Stanger Danger</a></span> <a data-action="follow" data-user="378404535" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-5450509664" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Benjamin</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-5611852775" data-role="relative-time" class="time-ago" title="Thursday, November 18, 2021 12:03 AM">3 years ago</a> <span> <span class="has-edit" data-role="has-edit">edited</span></span></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p><b>For the struggling ones in the future.</b><br>When you run the code below, it downloads the data-set in the directory mentioned in <b>"root" address. </b><br>Name any directory and it will create a folder with that name, then it proceeds to download the content "de-en.tgz" in <b>/myfolder/iwslt/</b><br><code>datasets.IWSLT.splits(exts=('.de', '.en'),<br>fields=(SRC, TGT),<br><b>root="/myfolder/",</b><br>filter_pred= lambda x: len(vars(x)['src']) &lt;= MAX_LEN and  len(vars(x)['trg']) &lt;= MAX_LEN)</code><br>Now, delete this old "de-en.tgz" file in /myfolder/iwslt/, and download tgz file mentioned above from the link, now after extracting, go to 2016-01/texts/de/en/<br><b>Copypaste the new tgz file into /myfolder/iwslt/.</b><br>Now, run the code again.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">1</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-5611852775" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/2kt5hbb" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:5611852775"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-5611852775-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-5450509664-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-5366537777-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-5350335350-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-5349873650-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4881933302"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4881933302"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/aimanmutasembellh/" data-action="profile" data-tab="" data-username="aimanmutasembellh" target="_blank" rel="noopener noreferrer" class="user user--refresh"><img data-role="user-avatar" data-user="136724852" src="./noavatar92.png" alt="Avatar" class="image-refresh"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/aimanmutasembellh/" data-action="profile" data-tab="" data-username="aimanmutasembellh" target="_blank" rel="noopener noreferrer">Aiman Mutasem-bellh</a></span> <a data-action="follow" data-user="136724852" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4881933302" data-role="relative-time" class="time-ago" title="Monday, April 20, 2020 10:16 AM">5 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks for the great post</p><p>I’m using standard TRANSFORMER for NMT and I’m going to train model Right to left. I have applied two ideas:</p><p>1. reversing the input text from left to Right, before feeding the data to <br> the encoder and decoder. (but the training process is still left to right)</p><p>2. reversing the embedding vector in the encoder and decoder. (Result is so boor, and the BLUE score is decreased by 15.6 points)</p><p>My question is what the optimal way to train model Right to Left?</p><p>Note: I have two versions of my model (RTL model) and (LTR model).</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">1</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4881933302" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/28qkrue" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4881933302"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4881933302-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4865841600"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4865841600"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_W64TGqfzwI/" data-action="profile" data-tab="" data-username="disqus_W64TGqfzwI" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>S</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_W64TGqfzwI/" data-action="profile" data-tab="" data-username="disqus_W64TGqfzwI" target="_blank" rel="noopener noreferrer">Salted Fish</a></span> <a data-action="follow" data-user="347313044" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4865841600" data-role="relative-time" class="time-ago" title="Wednesday, April 8, 2020 7:46 PM">5 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thank you for sharing so unselfishly. I have a question to ask you and I hope you can help me. About this tuple in Multihead attention:</p><p><code>query, key, value = /<br> [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)  <br>for l, x in zip(self.linears, (query, key, value))]</code></p><p>I've never seen a method that calls the element l(x) in a tuple like this, and I don't understand that very well.</p><p>For these three variables, “”query, key, value“”， I didn't see the input values before in the forward of Class multihead attention, but I found that they had values during the run. Where did they get their values from?</p><p>In my understanding, I think the values of these three variables “ query, key, value” are corresponding to the values of the first three self.linears, but it doesn't seem that way. I hope someone can give me some help to explain the details here. Thank you so much.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">1</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4865841600" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/28gzveo" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4865841600"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-5471043035"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="5471043035"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_52IybZTuQO/" data-action="profile" data-tab="" data-username="disqus_52IybZTuQO" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>I</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_52IybZTuQO/" data-action="profile" data-tab="" data-username="disqus_52IybZTuQO" target="_blank" rel="noopener noreferrer">ice ice</a></span> <a data-action="follow" data-user="372903320" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4865841600" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Salted Fish</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-5471043035" data-role="relative-time" class="time-ago" title="Tuesday, July 27, 2021 6:03 PM">3 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Your understanding is right. Q,K,V are actually corresponding to the values of the first three self.linears.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">0</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-5471043035" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/2ihbftn" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:5471043035"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-5471043035-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4865841600-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4683799335"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4683799335"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_jVxRqA9Gkm/" data-action="profile" data-tab="" data-username="disqus_jVxRqA9Gkm" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>A</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_jVxRqA9Gkm/" data-action="profile" data-tab="" data-username="disqus_jVxRqA9Gkm" target="_blank" rel="noopener noreferrer">Anshul Shah</a></span> <a data-action="follow" data-user="340520918" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4683799335" data-role="relative-time" class="time-ago" title="Sunday, November 10, 2019 10:26 AM">5 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks for the awesome post!<br>I have a question: there seems to be a small issue in the SimpleLossCompute function. loss.backward() accumulates gradient during evaluation but, they won't be set to zero before the next step. Won't the next training step use gradients computed during evaluation ?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">1</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4683799335" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/25gm2p3" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4683799335"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4886246116"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4886246116"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_wGGAF5yQKM/" data-action="profile" data-tab="" data-username="disqus_wGGAF5yQKM" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>G</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_wGGAF5yQKM/" data-action="profile" data-tab="" data-username="disqus_wGGAF5yQKM" target="_blank" rel="noopener noreferrer">Gus Smith</a></span> <a data-action="follow" data-user="177403162" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4683799335" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Anshul Shah</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4886246116" data-role="relative-time" class="time-ago" title="Thursday, April 23, 2020 1:13 PM">5 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Yes, it happens as you stated. Running validation does increase the performance on the validation set compare to if one does not run the validation. See my post here on the same topic.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">0</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4886246116" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/28t57ms" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4886246116"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4886246116-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4683799335-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4227028789"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4227028789"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/farzadsharif/" data-action="profile" data-tab="" data-username="farzadsharif" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>F</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/farzadsharif/" data-action="profile" data-tab="" data-username="farzadsharif" target="_blank" rel="noopener noreferrer">Farzad Sharif</a></span> <a data-action="follow" data-user="224156259" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4227028789" data-role="relative-time" class="time-ago" title="Thursday, December 6, 2018 6:17 AM">6 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Hi, so I'm trying to train the model on the IWSLT en-de dataset on a 2 GPU machine each with 12G memory. But I am running out of GPU memory. Is this normal?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">1</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4227028789" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1xwnwad" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4227028789"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4918157349"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4918157349"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/aimanmutasembellh/" data-action="profile" data-tab="" data-username="aimanmutasembellh" target="_blank" rel="noopener noreferrer" class="user user--refresh"><img data-role="user-avatar" data-user="136724852" src="./noavatar92.png" alt="Avatar" class="image-refresh"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/aimanmutasembellh/" data-action="profile" data-tab="" data-username="aimanmutasembellh" target="_blank" rel="noopener noreferrer">Aiman Mutasem-bellh</a></span> <a data-action="follow" data-user="136724852" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4227028789" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Farzad Sharif</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4918157349" data-role="relative-time" class="time-ago" title="Monday, May 18, 2020 9:28 PM">5 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Hello Mr. Farzad. Kindlly, did you fix this issue? :)</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">1</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4918157349" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/29c56hx" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4918157349"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4918157349-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-5382366618"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="5382366618"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_gDirQgYlPU/" data-action="profile" data-tab="" data-username="disqus_gDirQgYlPU" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>Y</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_gDirQgYlPU/" data-action="profile" data-tab="" data-username="disqus_gDirQgYlPU" target="_blank" rel="noopener noreferrer">yulong li</a></span> <a data-action="follow" data-user="368182855" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4227028789" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Farzad Sharif</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-5382366618" data-role="relative-time" class="time-ago" title="Friday, May 14, 2021 3:21 AM">4 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>In case other people having the same issue. In run_epoch function, should use total_loss += float(loss) instead of total_loss += loss. Otherwise may have CUDA out of memory error. <br>Explanation see: <a href="https://disq.us/url?url=https%3A%2F%2Fpytorch.org%2Fdocs%2Fstable%2Fnotes%2Ffaq.html%3AvbhubPu_TN8sWtqWT1LX4hNEyoA&amp;cuid=5460729" rel="nofollow noopener" title="https://pytorch.org/docs/stable/notes/faq.html">https://pytorch.org/docs/st...</a></p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">0</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-5382366618" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/2h0isnu" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:5382366618"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-5382366618-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4227028789-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3845808761"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="3845808761"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_htlIvpMsai/" data-action="profile" data-tab="" data-username="disqus_htlIvpMsai" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>주</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_htlIvpMsai/" data-action="profile" data-tab="" data-username="disqus_htlIvpMsai" target="_blank" rel="noopener noreferrer">주말이다</a></span> <a data-action="follow" data-user="194386225" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-3845808761" data-role="relative-time" class="time-ago" title="Monday, April 9, 2018 1:28 PM">7 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Thanks!</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">1</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-3845808761" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1rlp115" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3845808761"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3845808761-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-3873740919"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="3873740919"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_htlIvpMsai/" data-action="profile" data-tab="" data-username="disqus_htlIvpMsai" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>주</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_htlIvpMsai/" data-action="profile" data-tab="" data-username="disqus_htlIvpMsai" target="_blank" rel="noopener noreferrer">주말이다</a></span> <a data-action="follow" data-user="194386225" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-3873740919" data-role="relative-time" class="time-ago" title="Thursday, April 26, 2018 11:09 PM">7 years ago</a> <span> <span class="has-edit" data-role="has-edit">edited</span></span></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>I have a question about class SublayerConnection.<br>the comment says "for code simplicity the norm is first as opposed to last."<br>But the loss wont go down below 4~5 when the norm is applied after residual layer in the decoder, same as described in the paper.<br>Do you have any idea of this phenomenon?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-2" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">2</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-2" data-action="downvote" title="" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">2</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-3873740919" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1s2bpmf" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:3873740919"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4150550608"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4150550608"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_5iwwxhLeGL/" data-action="profile" data-tab="" data-username="disqus_5iwwxhLeGL" target="_blank" rel="noopener noreferrer" class="user user--refresh"><img data-role="user-avatar" data-user="323780185" src="./noavatar92.png" alt="Avatar" class="image-refresh"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_5iwwxhLeGL/" data-action="profile" data-tab="" data-username="disqus_5iwwxhLeGL" target="_blank" rel="noopener noreferrer">Yu-Tao</a></span> <a data-action="follow" data-user="323780185" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-3873740919" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> 주말이다</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4150550608" data-role="relative-time" class="time-ago" title="Thursday, October 18, 2018 1:44 PM">6 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>same question. <br>but why not change the code into <br>return self.norm(x + self.dropout(sublayer(x)))<br>this seems more like the original paper?</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-5" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">5</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4150550608" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1wn4pcg" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4150550608"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"><li class="post" id="post-4202213577"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4202213577"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_akZ5ASHeyH/" data-action="profile" data-tab="" data-username="disqus_akZ5ASHeyH" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>N</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_akZ5ASHeyH/" data-action="profile" data-tab="" data-username="disqus_akZ5ASHeyH" target="_blank" rel="noopener noreferrer">Neo li</a></span> <a data-action="follow" data-user="258332319" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4150550608" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Yu-Tao</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4202213577" data-role="relative-time" class="time-ago" title="Monday, November 19, 2018 8:22 PM">6 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>Yey, I have the same question. And  There new code in (<a href="https://disq.us/url?url=https%3A%2F%2Fgithub.com%2FOpenNMT%2FOpenNMT-py%2Fblob%2Fcd29c1dbfb35f4a2701ff52a1bf4e5bdcf02802e%2Fonmt%2Fencoders%2Ftransformer.py%23L48%29%3AkPEDwJxosNkT2LbhtOQLR49t7Oo&amp;cuid=5460729" rel="nofollow noopener" title="https://github.com/OpenNMT/OpenNMT-py/blob/cd29c1dbfb35f4a2701ff52a1bf4e5bdcf02802e/onmt/encoders/transformer.py#L48)">https://github.com/OpenNMT/...</a> still writing like that. Obviously, the formula they list does not match the code they have.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-6" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">6</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4202213577" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1xhw0qx" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4202213577"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4202213577-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4469709216"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4469709216"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/prashantserai/" data-action="profile" data-tab="" data-username="prashantserai" target="_blank" rel="noopener noreferrer" class="user user--refresh"><img data-role="user-avatar" data-user="105178679" src="./noavatar92.png" alt="Avatar" class="image-refresh"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/prashantserai/" data-action="profile" data-tab="" data-username="prashantserai" target="_blank" rel="noopener noreferrer">Prashant serai</a></span> <a data-action="follow" data-user="105178679" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4150550608" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Yu-Tao</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4469709216" data-role="relative-time" class="time-ago" title="Monday, May 20, 2019 10:37 PM">6 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>It appears that the code provided by original the authors of the paper differs in this matter from what's stated in the paper.<br>Here, they seem to implement the original authors' code instead of the original paper.</p><p>Reference:<br><a href="https://disq.us/url?url=https%3A%2F%2Fgithub.com%2FOpenNMT%2FOpenNMT-py%2Fissues%2F770%23issuecomment-398299135%3A09ugKrHc-S2FOi-LYJRpzwGY2mA&amp;cuid=5460729" rel="nofollow noopener" target="_blank" title="https://github.com/OpenNMT/OpenNMT-py/issues/770#issuecomment-398299135">https://github.com/OpenNMT/...</a></p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-1" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">1</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4469709216" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/21x5dpc" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4469709216"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4469709216-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4569337441"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4569337441"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/willprice94/" data-action="profile" data-tab="" data-username="willprice94" target="_blank" rel="noopener noreferrer" class="user user--refresh"><img data-role="user-avatar" data-user="21431364" src="./noavatar92.png" alt="Avatar" class="image-refresh"></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/willprice94/" data-action="profile" data-tab="" data-username="willprice94" target="_blank" rel="noopener noreferrer">willprice94</a></span> <a data-action="follow" data-user="21431364" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4150550608" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> Yu-Tao</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4569337441" data-role="relative-time" class="time-ago" title="Wednesday, August 7, 2019 11:02 PM">5 years ago</a> <span> <span class="has-edit" data-role="has-edit">edited</span></span></span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p>I too was confused about this, but it seems they are true to the implementation in T2T (<a href="https://disq.us/url?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensor2tensor%2Fblob%2F43be271c8a3fa06cb06b5147f044cbdc8bb77535%2Ftensor2tensor%2Fmodels%2Ftransformer.py%23L1799-L1800%3AlvQ3PVGJTkoDJWSy8faq-PLPCas&amp;cuid=5460729" rel="nofollow noopener" target="_blank" title="https://github.com/tensorflow/tensor2tensor/blob/43be271c8a3fa06cb06b5147f044cbdc8bb77535/tensor2tensor/models/transformer.py#L1799-L1800">https://github.com/tensorfl...</a> specifies the hyperparameters for the pre and post layer transformations applied by <a href="https://disq.us/url?url=https%3A%2F%2Fgithub.com%2Ftensorflow%2Ftensor2tensor%2Fblob%2F43be271c8a3fa06cb06b5147f044cbdc8bb77535%2Ftensor2tensor%2Flayers%2Fcommon_layers.py%23L862%29%3APcLTR_7HqSnJa4tSB9Ba8KMyPHc&amp;cuid=5460729" rel="nofollow noopener" target="_blank" title="https://github.com/tensorflow/tensor2tensor/blob/43be271c8a3fa06cb06b5147f044cbdc8bb77535/tensor2tensor/layers/common_layers.py#L862)">https://github.com/tensorfl...</a>. The description in the paper is inconsistent with the implementation.</p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-0" data-action="upvote" title="Vote up" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">0</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-0" data-action="downvote" title="Vote down" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">0</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4569337441" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/23kgrc1" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4569337441"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4569337441-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4150550608-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li><li class="post" id="post-4202228717"><div role="alert"></div><div data-role="post-content" class="post-content" tabindex="0"><div class="indicator"></div><span class="pinned-icon"></span><ul class="post-menu dropdown post-menu--refresh" data-role="menu" data-view-id="post-menu" data-post-id="4202228717"><li class="post-menu-item collapse"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Collapse" name="Collapse"><span>−</span></a></li><li class="post-menu-item expand"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="collapse" title="Expand" name="Collapse"><span>+</span></a></li><li class=" post-menu-item" role="menuitem"><a class="dropdown-toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="flag" data-role="flag" title="Flag as inappropriate"><i aria-hidden="true" class="icon icon-flag"></i></a></li></ul><div class="avatar hovercard"><a href="https://disqus.com/by/disqus_akZ5ASHeyH/" data-action="profile" data-tab="" data-username="disqus_akZ5ASHeyH" target="_blank" rel="noopener noreferrer" class="user user--refresh"><div>N</div></a></div><div class="post-body"><header class="comment__header"><span class="post-byline"><span> <span class="author publisher-anchor-color"><a href="https://disqus.com/by/disqus_akZ5ASHeyH/" data-action="profile" data-tab="" data-username="disqus_akZ5ASHeyH" target="_blank" rel="noopener noreferrer">Neo li</a></span> <a data-action="follow" data-user="258332319" class="follow-user-container" tabindex="0"><span class="follow-user" aria-label="Follow" title="Follow"></span></a></span><span class="parent-link-container"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-3873740919" class="parent-link" data-role="parent-link"><i aria-label="in reply to" class="icon-forward" title="in reply to"></i> 주말이다</a></span></span>  <span class="post-meta" style="display: block;"> <a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html#comment-4202228717" data-role="relative-time" class="time-ago" title="Monday, November 19, 2018 8:38 PM">6 years ago</a> </span> </header><div class="post-body-inner"><div class="post-message-container" data-role="message-container"><div class="publisher-anchor-color" data-role="message-content"><div class="post-message " data-role="message" dir="auto"><div><p><a href="https://disq.us/url?url=https%3A%2F%2Fgithub.com%2FOpenNMT%2FOpenNMT-py%2Fissues%2F770%3AP4vM-d65dWitBlBZMRrqtd4pcFw&amp;cuid=5460729" rel="nofollow noopener" title="https://github.com/OpenNMT/OpenNMT-py/issues/770">https://github.com/OpenNMT/...</a></p></div></div><span class="post-media"><ul data-role="post-media-list"></ul></span></div></div><a class="see-more hidden" title="see more" data-action="see-more">see more</a></div><footer class="comment__footer"><menu class="comment-footer__menu"><li class="voting" data-role="voting"><div class="post-votes"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-up  count-2" data-action="upvote" title="" name="Vote up"><span class="control"></span> <span class="updatable count" data-role="likes">2</span></a><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="vote-down  count-1" data-action="downvote" title="" name="Vote down"><span class="control"></span> <span class="updatable count" data-role="dislikes">1</span></a></div></li><li class="reply" data-role="reply-link"><a class="comment-footer__action" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="reply"><span class="text">Reply</span></a></li><li id="comment__share-4202228717" class="comment__share"><a class="toggle" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="expand-share"><i class="icon icon-share"></i><span class="text">Share ›</span></a><ul class="comment-share__buttons"><div class="comment-share__social-share-buttons"><li class="twitter share__button-container"><button class="share__button icon icon-twitter-x" data-action="share:twitter" aria-label="Share comment on X (Twitter)"></button></li><li class="facebook share__button-container"><button class="share__button icon icon-facebook" data-action="share:facebook" aria-label="Share comment on Facebook"></button></li></div><li class="link share__button-container"><button class="share__button icon icon-link" value="http://disq.us/p/1xhwcfh" name="Link" title="Click to copy post link" data-action="copy-link" aria-label="Copy link to comment"></button><input class="share__button link_url" name="Link" title="Click to copy post link" data-action="copy-link" readonly=""></li></ul></li><li class="realtime" data-role="realtime-notification:4202228717"><span class="realtime-replies realtime-replies--refresh icon icon-pencil" style="display: none;"></span><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" class="realtime-button realtime-button--refresh" style="display: none;"></a></li></menu></footer></div><div class="moderate-form blacklist-form" data-role="blacklist-form"></div><div class="moderate-form flag-form" data-role="flagging-form"></div><div class="badges-form" data-role="badges-form"></div><div class="reply-form-container" data-role="reply-form"></div></div><div class="children"><ul data-role="children"></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-4202228717-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="show-children-wrapper hidden"><a class="show-children" id="post-3873740919-show-children" data-action="show-children" href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#">Show more replies</a></div></div></li></ul><div class="load-more-refresh load-more-refresh--v2" data-role="more" style=""><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" data-action="more-posts" class="btn load-more-refresh__button ">Load more comments</a></div></div></section><div id="placement-bottom" data-tracking-area="discovery-south"></div><footer id="footer" data-tracking-area="footer" class="disqus-footer__wrapper disqus-footer__wrapper--refresh"><div><div class="disqus-footer disqus-footer--refresh"><ul class="disqus-footer__list"><li id="thread-subscribe-button" class="email disqus-footer__item disqus-footer__item--refresh"><div class="default"><a href="https://disqus.com/embed/comments/?base=default&amp;f=harvard-nlp&amp;t_u=https%3A%2F%2Fnlp.seas.harvard.edu%2F2018%2F04%2F03%2Fattention.html&amp;t_d=The%20Annotated%20Transformer&amp;t_t=The%20Annotated%20Transformer&amp;s_o=default#" rel="nofollow" data-action="subscribe" title="Subscribe and get email updates from this discussion" class="disqus-footer__link disqus-footer__link--refresh"><div class="icon-wrapper"><i aria-hidden="true" class="icon-subscribe-refresh"></i></div><i aria-hidden="true" class="icon icon-checkmark"></i><span id="thread-subscribe-text-default" class="text-item">Subscribe</span><span id="thread-subscribe-text-subscribed" class="text-item hidden">Subscribed</span></a></div></li><li class="privacy disqus-footer__item disqus-footer__item--refresh"><a href="https://disqus.com/privacy-policy" rel="nofollow noopener noreferrer" target="_blank" class="disqus-footer__link disqus-footer__link--refresh" title="Privacy"><div class="icon-wrapper"><i aria-hidden="true" class="icon-privacy-refresh"></i></div><span class="text-item">Privacy</span></a></li><li class="do-not-sell disqus-footer__item disqus-footer__item--refresh"><a href="https://disqus.com/data-sharing-settings/" rel="nofollow noopener noreferrer" target="_blank" class="disqus-footer__link disqus-footer__link--refresh"><div class="icon-wrapper"><i aria-hidden="true" class="icon-warning-refresh"></i></div><span class="text-item">Do Not Sell My Data</span></a></li></ul><span class="disqus-footer__logo"><a href="https://disqus.com/" rel="nofollow" title="Powered by Disqus" class="disqus-footer__link disqus-footer__link--refresh">Powered by Disqus</a></span></div></div></footer></div></div></div><div id="embed_v2-root"></div></div><div id="fb-root" class=" fb_reset"><div style="position: absolute; top: -10000px; width: 0px; height: 0px;"><div></div></div></div><iframe id="ssIFrame_google" sandbox="allow-scripts allow-same-origin allow-storage-access-by-user-activation" allow="identity-credentials-get" aria-hidden="true" frame-border="0" src="./iframe.html" style="position: absolute; width: 1px; height: 1px; inset: -9999px; display: none;"></iframe><div id=":r5:" data-floating-ui-portal=""><div class="_overlay_1i4qh_1" tabindex="-1" id=":r3:" role="listbox" style="position: fixed; left: 0px; top: 0px; transform: translate(62.08px, 140.8px); will-change: transform; width: 712.4px;"></div></div><iframe src="./pixel.html" style="display: none;"></iframe><img src="./saved_resource" style="display: none;"><iframe src="./sync.html" style="display: none;"></iframe></body></html>